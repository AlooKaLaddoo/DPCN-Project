{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f86bfc5",
   "metadata": {},
   "source": [
    "# Granger Causality Analysis - Infant EEG Dataset\n",
    "\n",
    "This notebook implements Granger causality analysis for the infant resting state EEG dataset (103 subjects, 1-4 sessions each).\n",
    "\n",
    "## Phase 1: Setup & Data Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971b7f5",
   "metadata": {},
   "source": [
    "## Step 1: Configuration Parameters\n",
    "\n",
    "All tunable parameters are centralized here for easy adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25f8da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directories created:\n",
      "  individual: /home/alookaladdoo/DPCN-Project/results/individual\n",
      "  group: /home/alookaladdoo/DPCN-Project/results/group\n",
      "  qc: /home/alookaladdoo/DPCN-Project/results/quality_control\n",
      "  logs: /home/alookaladdoo/DPCN-Project/results/logs\n",
      "  plots: /home/alookaladdoo/DPCN-Project/results/plots\n",
      "\n",
      "Preprocessing parameters set:\n",
      "  highpass_freq: 0.5\n",
      "  lowpass_freq: 30.0\n",
      "  notch_freq: 60.0\n",
      "  filter_method: fir\n",
      "  reference: average\n",
      "  re_reference: True\n",
      "  bad_channel_threshold: 0.3\n",
      "  interpolate_bad: True\n",
      "\n",
      "Segmentation parameters set:\n",
      "  window_length: 10.0\n",
      "  window_overlap: 0.5\n",
      "  min_segment_duration: 5.0\n",
      "  use_eyes_closed_only: True\n",
      "  reject_artifacts: True\n",
      "\n",
      "Granger Causality parameters set:\n",
      "  model_order_method: aic\n",
      "  max_order: 50\n",
      "  min_order: 1\n",
      "  fixed_order: None\n",
      "  gc_method: pairwise\n",
      "  compute_spectral_gc: True\n",
      "  Frequency bands:\n",
      "    delta: 0.5-4 Hz\n",
      "    theta: 4-8 Hz\n",
      "    alpha: 8-13 Hz\n",
      "    beta: 13-30 Hz\n",
      "\n",
      "Statistical parameters set:\n",
      "  significance_threshold: 0.05\n",
      "  correction_method: fdr_bh\n",
      "  n_permutations: 1000\n",
      "  confidence_level: 0.95\n",
      "  random_seed: 42\n",
      "\n",
      "Analysis parameters set:\n",
      "  analyze_all_pairs: True\n",
      "  channel_pairs: None\n",
      "  compute_bidirectional: True\n",
      "  age_bins: [0, 0.25, 0.5, 0.75, 1.0]\n",
      "\n",
      "Visualization parameters set:\n",
      "  colormap: viridis\n",
      "  figure_dpi: 300\n",
      "  figure_format: png\n",
      "  figure_size: (10, 8)\n",
      "  save_individual_plots: True\n",
      "  save_group_plots: True\n",
      "\n",
      "Computational parameters set:\n",
      "  n_jobs: -1\n",
      "  verbose: 1\n",
      "  memory_limit_gb: None\n",
      "\n",
      "================================================================================\n",
      "CONFIGURATION COMPLETE\n",
      "================================================================================\n",
      "Dataset path: /home/alookaladdoo/DPCN-Project/Dataset\n",
      "Output path: /home/alookaladdoo/DPCN-Project/results\n",
      "Random seed: 42\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. PATH PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "DATASET_BASE_PATH = Path('/home/alookaladdoo/DPCN-Project/Dataset')\n",
    "OUTPUT_BASE_PATH = Path('/home/alookaladdoo/DPCN-Project/results')\n",
    "DERIVATIVES_PATH = DATASET_BASE_PATH / 'derivatives' / 'NeuronicEEG'\n",
    "\n",
    "# Create output directory structure\n",
    "OUTPUT_DIRS = {\n",
    "    'individual': OUTPUT_BASE_PATH / 'individual',\n",
    "    'group': OUTPUT_BASE_PATH / 'group',\n",
    "    'qc': OUTPUT_BASE_PATH / 'quality_control',\n",
    "    'logs': OUTPUT_BASE_PATH / 'logs',\n",
    "    'plots': OUTPUT_BASE_PATH / 'plots'\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Output directories created:\")\n",
    "for name, path in OUTPUT_DIRS.items():\n",
    "    print(f\"  {name}: {path}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. PREPROCESSING PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "PREPROCESS_PARAMS = {\n",
    "    # Filter settings\n",
    "    'highpass_freq': 0.5,      # Hz - High-pass filter cutoff\n",
    "    'lowpass_freq': 30.0,      # Hz - Low-pass filter cutoff\n",
    "    'notch_freq': 60.0,        # Hz - Notch filter for power line noise\n",
    "    'filter_method': 'fir',    # Filter method: 'fir' or 'iir'\n",
    "    \n",
    "    # Reference settings\n",
    "    'reference': 'average',    # 'average', 'common', or list of channel names\n",
    "    're_reference': True,      # Whether to re-reference data\n",
    "    \n",
    "    # Data quality\n",
    "    'bad_channel_threshold': 0.3,  # Fraction of bad data to mark channel as bad\n",
    "    'interpolate_bad': True,       # Whether to interpolate bad channels\n",
    "}\n",
    "\n",
    "print(\"\\nPreprocessing parameters set:\")\n",
    "for key, value in PREPROCESS_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. SEGMENTATION PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "SEGMENT_PARAMS = {\n",
    "    'window_length': 10.0,     # seconds - Length of each analysis window\n",
    "    'window_overlap': 0.5,     # 0-1 - Overlap between windows (50%)\n",
    "    'min_segment_duration': 5.0,  # seconds - Minimum acceptable segment length\n",
    "    'use_eyes_closed_only': True,  # Only analyze eyes-closed segments\n",
    "    'reject_artifacts': True,      # Use derivative annotations to reject artifacts\n",
    "}\n",
    "\n",
    "print(\"\\nSegmentation parameters set:\")\n",
    "for key, value in SEGMENT_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. GRANGER CAUSALITY PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "GC_PARAMS = {\n",
    "    # Model order selection\n",
    "    'model_order_method': 'aic',   # 'aic', 'bic', 'hqc', or 'fixed'\n",
    "    'max_order': 50,               # Maximum lag order to test\n",
    "    'min_order': 1,                # Minimum lag order to test\n",
    "    'fixed_order': None,           # Use this if model_order_method='fixed'\n",
    "    \n",
    "    # GC computation\n",
    "    'gc_method': 'pairwise',       # 'pairwise' or 'conditional' (multivariate)\n",
    "    'compute_spectral_gc': True,   # Whether to compute frequency-domain GC\n",
    "    \n",
    "    # Frequency bands for spectral analysis\n",
    "    'freq_bands': {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 30),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nGranger Causality parameters set:\")\n",
    "for key, value in GC_PARAMS.items():\n",
    "    if key != 'freq_bands':\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"  Frequency bands:\")\n",
    "for band, (low, high) in GC_PARAMS['freq_bands'].items():\n",
    "    print(f\"    {band}: {low}-{high} Hz\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. STATISTICAL PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "STAT_PARAMS = {\n",
    "    'significance_threshold': 0.05,    # p-value threshold\n",
    "    'correction_method': 'fdr_bh',     # 'fdr_bh', 'bonferroni', 'permutation'\n",
    "    'n_permutations': 100,            # Number of permutations for testing\n",
    "    'confidence_level': 0.95,          # Confidence interval level\n",
    "    'random_seed': 42,                 # For reproducibility\n",
    "}\n",
    "\n",
    "print(\"\\nStatistical parameters set:\")\n",
    "for key, value in STAT_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. ANALYSIS PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "ANALYSIS_PARAMS = {\n",
    "    'analyze_all_pairs': True,         # Analyze all channel pairs\n",
    "    'channel_pairs': None,             # Specific pairs to analyze (if not all)\n",
    "    'compute_bidirectional': True,     # Compute both X->Y and Y->X\n",
    "    'age_bins': [0, 0.25, 0.5, 0.75, 1.0],  # Age bins in years for stratification\n",
    "}\n",
    "\n",
    "print(\"\\nAnalysis parameters set:\")\n",
    "for key, value in ANALYSIS_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7. VISUALIZATION PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "VIZ_PARAMS = {\n",
    "    'colormap': 'viridis',             # Colormap for heatmaps\n",
    "    'figure_dpi': 300,                 # DPI for saved figures\n",
    "    'figure_format': 'png',            # 'png', 'pdf', 'svg'\n",
    "    'figure_size': (10, 8),            # Default figure size (width, height)\n",
    "    'save_individual_plots': True,     # Save plots for each subject\n",
    "    'save_group_plots': True,          # Save group-level plots\n",
    "}\n",
    "\n",
    "print(\"\\nVisualization parameters set:\")\n",
    "for key, value in VIZ_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8. COMPUTATIONAL PARAMETERS\n",
    "# ----------------------------------------------------------------------------\n",
    "COMPUTE_PARAMS = {\n",
    "    'n_jobs': -1,                      # Number of parallel jobs (-1 = all cores)\n",
    "    'verbose': 1,                      # Verbosity level (0, 1, 2)\n",
    "    'memory_limit_gb': None,           # Memory limit per process (None = no limit)\n",
    "}\n",
    "\n",
    "print(\"\\nComputational parameters set:\")\n",
    "for key, value in COMPUTE_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset path: {DATASET_BASE_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_BASE_PATH}\")\n",
    "print(f\"Random seed: {STAT_PARAMS['random_seed']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195d43c",
   "metadata": {},
   "source": [
    "## Step 2: Environment Setup\n",
    "\n",
    "Import all necessary libraries and verify their installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79216e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "================================================================================\n",
      "✓ Core libraries: numpy, pandas, scipy\n",
      "✓ MNE-Python version: 1.10.2\n",
      "✓ pyedflib available\n",
      "✓ statsmodels version: 0.14.5\n",
      "✓ matplotlib version: 3.10.6\n",
      "✓ seaborn version: 0.13.2\n",
      "✓ networkx version: 3.5\n",
      "✓ joblib available - 16 CPU cores detected\n",
      "✓ Logging configured\n",
      "✓ Random seed set to: 42\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT SETUP COMPLETE\n",
      "================================================================================\n",
      "✓ All critical libraries loaded successfully\n",
      "\n",
      "Optional libraries:\n",
      "  ✓ pyedflib\n",
      "  ✓ networkx\n",
      "  ✓ joblib\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "print(\"Importing libraries...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core Scientific Computing\n",
    "# ----------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "print(\"✓ Core libraries: numpy, pandas, scipy\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# EEG Processing\n",
    "# ----------------------------------------------------------------------------\n",
    "try:\n",
    "    import mne\n",
    "    print(f\"✓ MNE-Python version: {mne.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ MNE-Python not found - will need to install\")\n",
    "    mne = None\n",
    "\n",
    "try:\n",
    "    import pyedflib\n",
    "    print(f\"✓ pyedflib available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ pyedflib not found (optional - MNE can read EDF files)\")\n",
    "    pyedflib = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Statistical Modeling\n",
    "# ----------------------------------------------------------------------------\n",
    "try:\n",
    "    import statsmodels\n",
    "    from statsmodels.tsa.api import VAR\n",
    "    from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    print(f\"✓ statsmodels version: {statsmodels.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ statsmodels not found - will need to install\")\n",
    "    statsmodels = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Visualization\n",
    "# ----------------------------------------------------------------------------\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set matplotlib backend and style\n",
    "    matplotlib.use('Agg')  # Non-interactive backend for saving figures\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    print(f\"✓ matplotlib version: {matplotlib.__version__}\")\n",
    "    print(f\"✓ seaborn version: {sns.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Visualization libraries error: {e}\")\n",
    "    matplotlib = None\n",
    "    plt = None\n",
    "    sns = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Network Analysis\n",
    "# ----------------------------------------------------------------------------\n",
    "try:\n",
    "    import networkx as nx\n",
    "    print(f\"✓ networkx version: {nx.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ networkx not found - will need to install for network analysis\")\n",
    "    nx = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Parallel Processing\n",
    "# ----------------------------------------------------------------------------\n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "    import multiprocessing\n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    print(f\"✓ joblib available - {n_cores} CPU cores detected\")\n",
    "except ImportError:\n",
    "    print(\"⚠ joblib not found - parallel processing will be limited\")\n",
    "    Parallel = None\n",
    "    delayed = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Configure Logging\n",
    "# ----------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(OUTPUT_DIRS['logs'] / 'analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Logging configured\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Set Random Seeds for Reproducibility\n",
    "# ----------------------------------------------------------------------------\n",
    "np.random.seed(STAT_PARAMS['random_seed'])\n",
    "if mne is not None:\n",
    "    mne.set_log_level('WARNING')\n",
    "    \n",
    "print(f\"✓ Random seed set to: {STAT_PARAMS['random_seed']}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Environment Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check critical dependencies\n",
    "critical_libs = {\n",
    "    'numpy': np is not None,\n",
    "    'pandas': pd is not None,\n",
    "    'scipy': signal is not None,\n",
    "    'mne': mne is not None,\n",
    "    'statsmodels': statsmodels is not None,\n",
    "    'matplotlib': matplotlib is not None,\n",
    "}\n",
    "\n",
    "all_critical = all(critical_libs.values())\n",
    "\n",
    "if all_critical:\n",
    "    print(\"✓ All critical libraries loaded successfully\")\n",
    "else:\n",
    "    print(\"⚠ Missing critical libraries:\")\n",
    "    for lib, loaded in critical_libs.items():\n",
    "        if not loaded:\n",
    "            print(f\"  ✗ {lib}\")\n",
    "\n",
    "print(\"\\nOptional libraries:\")\n",
    "optional_libs = {\n",
    "    'pyedflib': pyedflib is not None,\n",
    "    'networkx': nx is not None,\n",
    "    'joblib': Parallel is not None,\n",
    "}\n",
    "for lib, loaded in optional_libs.items():\n",
    "    status = \"✓\" if loaded else \"✗\"\n",
    "    print(f\"  {status} {lib}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa192b9b",
   "metadata": {},
   "source": [
    "### Install Missing Dependencies (if needed)\n",
    "\n",
    "Run this cell only if the environment setup above shows missing critical libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c91d01f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install missing packages, uncomment the pip install commands above and run this cell.\n",
      "After installation, restart the kernel and re-run from Step 1.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell if you need to install missing packages\n",
    "\n",
    "# !pip install numpy pandas scipy matplotlib seaborn\n",
    "# !pip install mne statsmodels networkx joblib\n",
    "# !pip install pyedflib  # Optional but recommended\n",
    "\n",
    "print(\"To install missing packages, uncomment the pip install commands above and run this cell.\")\n",
    "print(\"After installation, restart the kernel and re-run from Step 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a035d7",
   "metadata": {},
   "source": [
    "## Step 3: Data Inventory\n",
    "\n",
    "Scan the dataset to create a comprehensive inventory of all subjects, sessions, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a4ae35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data discovery...\n",
      "================================================================================\n",
      "✓ Loaded participants.tsv: 103 subjects\n",
      "  - Female: 41\n",
      "  - Male: 62\n",
      "\n",
      "Scanning dataset directory structure...\n",
      "✓ Found 103 subject directories\n",
      "\n",
      "✓ Scanned 130 sessions across 103 subjects\n",
      "================================================================================\n",
      "\n",
      "✓ Scanned 130 sessions across 103 subjects\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA DISCOVERY AND INVENTORY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting data discovery...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load participants metadata\n",
    "# ----------------------------------------------------------------------------\n",
    "participants_file = DATASET_BASE_PATH / 'participants.tsv'\n",
    "if participants_file.exists():\n",
    "    participants_df = pd.read_csv(participants_file, sep='\\t')\n",
    "    print(f\"✓ Loaded participants.tsv: {len(participants_df)} subjects\")\n",
    "    print(f\"  - Female: {(participants_df['sex'] == 'F').sum()}\")\n",
    "    print(f\"  - Male: {(participants_df['sex'] == 'M').sum()}\")\n",
    "else:\n",
    "    print(\"✗ participants.tsv not found!\")\n",
    "    participants_df = None\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. Scan dataset directory structure\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\nScanning dataset directory structure...\")\n",
    "\n",
    "subject_dirs = sorted([d for d in DATASET_BASE_PATH.glob('sub-*') if d.is_dir()])\n",
    "print(f\"✓ Found {len(subject_dirs)} subject directories\")\n",
    "\n",
    "# Initialize inventory list\n",
    "inventory_data = []\n",
    "\n",
    "# Scan each subject\n",
    "for subject_dir in subject_dirs:\n",
    "    subject_id = subject_dir.name\n",
    "    \n",
    "    # Find all sessions for this subject\n",
    "    session_dirs = sorted([d for d in subject_dir.glob('ses-*') if d.is_dir()])\n",
    "    \n",
    "    for session_dir in session_dirs:\n",
    "        session_id = session_dir.name\n",
    "        \n",
    "        # Initialize record\n",
    "        record = {\n",
    "            'subject_id': subject_id,\n",
    "            'session_id': session_id,\n",
    "            'sex': None,\n",
    "            'age_years': None,\n",
    "            'age_months': None,\n",
    "            'eeg_file_path': None,\n",
    "            'duration_sec': None,\n",
    "            'n_channels': None,\n",
    "            'sampling_freq': None,\n",
    "            'has_events': False,\n",
    "            'has_annotations': False,\n",
    "            'has_derivatives': False,\n",
    "            'file_exists': False,\n",
    "            'session_count_for_subject': len(session_dirs)\n",
    "        }\n",
    "        \n",
    "        # Get sex from participants.tsv\n",
    "        if participants_df is not None:\n",
    "            subject_row = participants_df[participants_df['participant_id'] == subject_id]\n",
    "            if not subject_row.empty:\n",
    "                record['sex'] = subject_row.iloc[0]['sex']\n",
    "        \n",
    "        # Look for scans.tsv to get age\n",
    "        scans_file = session_dir / f\"{subject_id}_{session_id}_scans.tsv\"\n",
    "        if scans_file.exists():\n",
    "            scans_df = pd.read_csv(scans_file, sep='\\t')\n",
    "            if 'age_acq_time' in scans_df.columns and not scans_df.empty:\n",
    "                age_years = scans_df.iloc[0]['age_acq_time']\n",
    "                record['age_years'] = age_years\n",
    "                record['age_months'] = age_years * 12  # Convert to months\n",
    "        \n",
    "        # Look for EEG data file\n",
    "        eeg_dir = session_dir / 'eeg'\n",
    "        if eeg_dir.exists():\n",
    "            eeg_files = list(eeg_dir.glob(f\"{subject_id}_{session_id}_task-EEG_eeg.edf\"))\n",
    "            if eeg_files:\n",
    "                eeg_file = eeg_files[0]\n",
    "                record['eeg_file_path'] = str(eeg_file)\n",
    "                record['file_exists'] = True\n",
    "                \n",
    "                # Try to read basic info from EEG file\n",
    "                try:\n",
    "                    raw = mne.io.read_raw_edf(eeg_file, preload=False, verbose='ERROR')\n",
    "                    record['duration_sec'] = raw.times[-1]\n",
    "                    record['n_channels'] = len(raw.ch_names)\n",
    "                    record['sampling_freq'] = raw.info['sfreq']\n",
    "                    del raw  # Free memory\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not read {eeg_file}: {e}\")\n",
    "            \n",
    "            # Check for events file\n",
    "            events_files = list(eeg_dir.glob(f\"{subject_id}_{session_id}_task-EEG_events.tsv\"))\n",
    "            record['has_events'] = len(events_files) > 0\n",
    "        \n",
    "        # Check for derivative annotations\n",
    "        deriv_dir = DERIVATIVES_PATH / subject_id / session_id / 'eeg'\n",
    "        if deriv_dir.exists():\n",
    "            annot_files = list(deriv_dir.glob(f\"{subject_id}_{session_id}_task-EEG_annotations.tsv\"))\n",
    "            record['has_derivatives'] = len(annot_files) > 0\n",
    "            record['has_annotations'] = record['has_derivatives']\n",
    "        \n",
    "        inventory_data.append(record)\n",
    "\n",
    "# Create inventory DataFrame\n",
    "inventory_df = pd.DataFrame(inventory_data)\n",
    "\n",
    "print(f\"\\n✓ Scanned {len(inventory_df)} sessions across {len(subject_dirs)} subjects\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de891754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary Statistics\n",
      "================================================================================\n",
      "Total Subjects: 103\n",
      "Total Sessions: 130\n",
      "Valid EEG Files: 130\n",
      "Missing Files: 0\n",
      "\n",
      "Session Distribution:\n",
      "  Subjects with 1 session(s): 81\n",
      "  Subjects with 2 session(s): 18\n",
      "  Subjects with 3 session(s): 3\n",
      "  Subjects with 4 session(s): 1\n",
      "\n",
      "Sex Distribution:\n",
      "  M: 79 sessions\n",
      "  F: 51 sessions\n",
      "\n",
      "Age Distribution:\n",
      "  Mean: 5.36 months (0.45 years)\n",
      "  Std Dev: 3.09 months\n",
      "  Range: 0.26 - 12.56 months\n",
      "  Median: 4.79 months\n",
      "\n",
      "Recording Duration:\n",
      "  Mean: 632.46 seconds (10.54 minutes)\n",
      "  Range: 32.00 - 1783.99 seconds\n",
      "\n",
      "Channel Information:\n",
      "  Most common channel count: 19\n",
      "  Channel count range: 19 - 24\n",
      "\n",
      "Annotations:\n",
      "  Sessions with events: 130\n",
      "  Sessions with derivative annotations: 130\n",
      "\n",
      "Data Completeness:\n",
      "  EEG file: 100.0%\n",
      "  Age info: 100.0%\n",
      "  Sex info: 100.0%\n",
      "  Events: 100.0%\n",
      "  Annotations: 100.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nDataset Summary Statistics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Basic counts\n",
    "total_subjects = inventory_df['subject_id'].nunique()\n",
    "total_sessions = len(inventory_df)\n",
    "valid_sessions = inventory_df['file_exists'].sum()\n",
    "\n",
    "print(f\"Total Subjects: {total_subjects}\")\n",
    "print(f\"Total Sessions: {total_sessions}\")\n",
    "print(f\"Valid EEG Files: {valid_sessions}\")\n",
    "print(f\"Missing Files: {total_sessions - valid_sessions}\")\n",
    "\n",
    "# Session distribution\n",
    "print(\"\\nSession Distribution:\")\n",
    "session_counts = inventory_df.groupby('subject_id')['session_id'].count().value_counts().sort_index()\n",
    "for n_sessions, n_subjects in session_counts.items():\n",
    "    print(f\"  Subjects with {n_sessions} session(s): {n_subjects}\")\n",
    "\n",
    "# Sex distribution\n",
    "if inventory_df['sex'].notna().any():\n",
    "    print(\"\\nSex Distribution:\")\n",
    "    sex_counts = inventory_df['sex'].value_counts()\n",
    "    for sex, count in sex_counts.items():\n",
    "        print(f\"  {sex}: {count} sessions\")\n",
    "\n",
    "# Age distribution\n",
    "if inventory_df['age_months'].notna().any():\n",
    "    print(\"\\nAge Distribution:\")\n",
    "    age_data = inventory_df.dropna(subset=['age_months'])\n",
    "    print(f\"  Mean: {age_data['age_months'].mean():.2f} months ({age_data['age_years'].mean():.2f} years)\")\n",
    "    print(f\"  Std Dev: {age_data['age_months'].std():.2f} months\")\n",
    "    print(f\"  Range: {age_data['age_months'].min():.2f} - {age_data['age_months'].max():.2f} months\")\n",
    "    print(f\"  Median: {age_data['age_months'].median():.2f} months\")\n",
    "\n",
    "# Recording duration\n",
    "if inventory_df['duration_sec'].notna().any():\n",
    "    print(\"\\nRecording Duration:\")\n",
    "    dur_data = inventory_df.dropna(subset=['duration_sec'])\n",
    "    print(f\"  Mean: {dur_data['duration_sec'].mean():.2f} seconds ({dur_data['duration_sec'].mean()/60:.2f} minutes)\")\n",
    "    print(f\"  Range: {dur_data['duration_sec'].min():.2f} - {dur_data['duration_sec'].max():.2f} seconds\")\n",
    "\n",
    "# Channel count\n",
    "if inventory_df['n_channels'].notna().any():\n",
    "    print(\"\\nChannel Information:\")\n",
    "    chan_data = inventory_df.dropna(subset=['n_channels'])\n",
    "    print(f\"  Most common channel count: {chan_data['n_channels'].mode().iloc[0]:.0f}\")\n",
    "    print(f\"  Channel count range: {chan_data['n_channels'].min():.0f} - {chan_data['n_channels'].max():.0f}\")\n",
    "\n",
    "# Annotations\n",
    "print(\"\\nAnnotations:\")\n",
    "print(f\"  Sessions with events: {inventory_df['has_events'].sum()}\")\n",
    "print(f\"  Sessions with derivative annotations: {inventory_df['has_derivatives'].sum()}\")\n",
    "\n",
    "# Data completeness\n",
    "print(\"\\nData Completeness:\")\n",
    "completeness = {\n",
    "    'EEG file': (inventory_df['file_exists'].sum() / len(inventory_df) * 100),\n",
    "    'Age info': (inventory_df['age_months'].notna().sum() / len(inventory_df) * 100),\n",
    "    'Sex info': (inventory_df['sex'].notna().sum() / len(inventory_df) * 100),\n",
    "    'Events': (inventory_df['has_events'].sum() / len(inventory_df) * 100),\n",
    "    'Annotations': (inventory_df['has_annotations'].sum() / len(inventory_df) * 100),\n",
    "}\n",
    "for field, pct in completeness.items():\n",
    "    print(f\"  {field}: {pct:.1f}%\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b85d4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved inventory to: /home/alookaladdoo/DPCN-Project/results/subject_session_inventory.csv\n",
      "✓ Saved summary statistics to: /home/alookaladdoo/DPCN-Project/results/data_summary_statistics.txt\n",
      "\n",
      "First 10 rows of inventory:\n",
      "      subject_id session_id sex  age_years  age_months                                                                                         eeg_file_path  duration_sec  n_channels  sampling_freq  has_events  has_annotations  has_derivatives  file_exists  session_count_for_subject\n",
      "0  sub-NORB00001      ses-1   M     0.4071      4.8852  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_eeg.edf       713.995          21          200.0        True             True             True         True                          1\n",
      "1  sub-NORB00002      ses-1   M     0.3907      4.6884  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00002/ses-1/eeg/sub-NORB00002_ses-1_task-EEG_eeg.edf       527.995          19          200.0        True             True             True         True                          1\n",
      "2  sub-NORB00003      ses-1   F     0.6503      7.8036  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00003/ses-1/eeg/sub-NORB00003_ses-1_task-EEG_eeg.edf      1783.995          19          200.0        True             True             True         True                          1\n",
      "3  sub-NORB00004      ses-1   M     0.2951      3.5412  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00004/ses-1/eeg/sub-NORB00004_ses-1_task-EEG_eeg.edf       743.995          19          200.0        True             True             True         True                          1\n",
      "4  sub-NORB00005      ses-1   M     0.8224      9.8688  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00005/ses-1/eeg/sub-NORB00005_ses-1_task-EEG_eeg.edf       775.995          19          200.0        True             True             True         True                          1\n",
      "5  sub-NORB00006      ses-1   M     0.9727     11.6724  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00006/ses-1/eeg/sub-NORB00006_ses-1_task-EEG_eeg.edf       599.995          19          200.0        True             True             True         True                          1\n",
      "6  sub-NORB00007      ses-1   M     0.5273      6.3276  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00007/ses-1/eeg/sub-NORB00007_ses-1_task-EEG_eeg.edf       791.995          19          200.0        True             True             True         True                          1\n",
      "7  sub-NORB00008      ses-1   M     0.1585      1.9020  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00008/ses-1/eeg/sub-NORB00008_ses-1_task-EEG_eeg.edf       543.995          19          200.0        True             True             True         True                          1\n",
      "8  sub-NORB00009      ses-1   F     0.0792      0.9504  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00009/ses-1/eeg/sub-NORB00009_ses-1_task-EEG_eeg.edf       567.995          19          200.0        True             True             True         True                          1\n",
      "9  sub-NORB00010      ses-1   M     0.4945      5.9340  /home/alookaladdoo/DPCN-Project/Dataset/sub-NORB00010/ses-1/eeg/sub-NORB00010_ses-1_task-EEG_eeg.edf       735.995          19          200.0        True             True             True         True                          1\n",
      "\n",
      "================================================================================\n",
      "DATA INVENTORY COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE INVENTORY TO FILE\n",
    "# ============================================================================\n",
    "\n",
    "# Save master inventory CSV\n",
    "inventory_csv_path = OUTPUT_BASE_PATH / 'subject_session_inventory.csv'\n",
    "inventory_df.to_csv(inventory_csv_path, index=False)\n",
    "print(f\"\\n✓ Saved inventory to: {inventory_csv_path}\")\n",
    "\n",
    "# Save summary statistics to text file\n",
    "summary_txt_path = OUTPUT_BASE_PATH / 'data_summary_statistics.txt'\n",
    "with open(summary_txt_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"INFANT EEG DATASET - SUMMARY STATISTICS\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Total Subjects: {total_subjects}\\n\")\n",
    "    f.write(f\"Total Sessions: {total_sessions}\\n\")\n",
    "    f.write(f\"Valid EEG Files: {valid_sessions}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Session Distribution:\\n\")\n",
    "    for n_sessions, n_subjects in session_counts.items():\n",
    "        f.write(f\"  Subjects with {n_sessions} session(s): {n_subjects}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    if inventory_df['sex'].notna().any():\n",
    "        f.write(\"Sex Distribution:\\n\")\n",
    "        sex_counts = inventory_df['sex'].value_counts()\n",
    "        for sex, count in sex_counts.items():\n",
    "            f.write(f\"  {sex}: {count} sessions\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    if inventory_df['age_months'].notna().any():\n",
    "        age_data = inventory_df.dropna(subset=['age_months'])\n",
    "        f.write(\"Age Distribution:\\n\")\n",
    "        f.write(f\"  Mean: {age_data['age_months'].mean():.2f} months\\n\")\n",
    "        f.write(f\"  Std Dev: {age_data['age_months'].std():.2f} months\\n\")\n",
    "        f.write(f\"  Range: {age_data['age_months'].min():.2f} - {age_data['age_months'].max():.2f} months\\n\")\n",
    "        f.write(f\"  Median: {age_data['age_months'].median():.2f} months\\n\\n\")\n",
    "    \n",
    "    if inventory_df['duration_sec'].notna().any():\n",
    "        dur_data = inventory_df.dropna(subset=['duration_sec'])\n",
    "        f.write(\"Recording Duration:\\n\")\n",
    "        f.write(f\"  Mean: {dur_data['duration_sec'].mean():.2f} seconds\\n\")\n",
    "        f.write(f\"  Range: {dur_data['duration_sec'].min():.2f} - {dur_data['duration_sec'].max():.2f} seconds\\n\\n\")\n",
    "    \n",
    "    f.write(\"Data Completeness:\\n\")\n",
    "    for field, pct in completeness.items():\n",
    "        f.write(f\"  {field}: {pct:.1f}%\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"✓ Saved summary statistics to: {summary_txt_path}\")\n",
    "\n",
    "# Display first few rows of inventory\n",
    "print(\"\\nFirst 10 rows of inventory:\")\n",
    "print(inventory_df.head(10).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA INVENTORY COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e60da",
   "metadata": {},
   "source": [
    "### Visualize Data Distribution\n",
    "\n",
    "Quick visualization of the dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f9dbd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved overview plot to: /home/alookaladdoo/DPCN-Project/results/plots/dataset_overview.png\n",
      "\n",
      "================================================================================\n",
      "VISUALIZATION COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/1923671248.py:87: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE DATA DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Dataset Overview - Infant EEG', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Session count distribution\n",
    "ax = axes[0, 0]\n",
    "session_counts.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "ax.set_xlabel('Number of Sessions')\n",
    "ax.set_ylabel('Number of Subjects')\n",
    "ax.set_title('Sessions per Subject')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Age distribution\n",
    "ax = axes[0, 1]\n",
    "if inventory_df['age_months'].notna().any():\n",
    "    age_data = inventory_df.dropna(subset=['age_months'])\n",
    "    ax.hist(age_data['age_months'], bins=20, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(age_data['age_months'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {age_data[\"age_months\"].mean():.1f}')\n",
    "    ax.set_xlabel('Age (months)')\n",
    "    ax.set_ylabel('Number of Sessions')\n",
    "    ax.set_title('Age Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No age data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "# 3. Recording duration distribution\n",
    "ax = axes[0, 2]\n",
    "if inventory_df['duration_sec'].notna().any():\n",
    "    dur_data = inventory_df.dropna(subset=['duration_sec'])\n",
    "    ax.hist(dur_data['duration_sec'] / 60, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Duration (minutes)')\n",
    "    ax.set_ylabel('Number of Sessions')\n",
    "    ax.set_title('Recording Duration')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No duration data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "# 4. Sex distribution\n",
    "ax = axes[1, 0]\n",
    "if inventory_df['sex'].notna().any():\n",
    "    sex_counts = inventory_df['sex'].value_counts()\n",
    "    colors = ['lightblue' if x == 'M' else 'pink' for x in sex_counts.index]\n",
    "    sex_counts.plot(kind='bar', ax=ax, color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('Sex')\n",
    "    ax.set_ylabel('Number of Sessions')\n",
    "    ax.set_title('Sex Distribution')\n",
    "    ax.set_xticklabels(sex_counts.index, rotation=0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No sex data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "# 5. Data completeness\n",
    "ax = axes[1, 1]\n",
    "completeness_df = pd.Series(completeness)\n",
    "colors_comp = ['green' if x > 90 else 'orange' if x > 70 else 'red' for x in completeness_df.values]\n",
    "completeness_df.plot(kind='barh', ax=ax, color=colors_comp, edgecolor='black')\n",
    "ax.set_xlabel('Completeness (%)')\n",
    "ax.set_title('Data Completeness')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 6. Channel count distribution\n",
    "ax = axes[1, 2]\n",
    "if inventory_df['n_channels'].notna().any():\n",
    "    chan_data = inventory_df.dropna(subset=['n_channels'])\n",
    "    chan_counts = chan_data['n_channels'].value_counts().sort_index()\n",
    "    chan_counts.plot(kind='bar', ax=ax, color='mediumpurple', edgecolor='black')\n",
    "    ax.set_xlabel('Number of Channels')\n",
    "    ax.set_ylabel('Number of Sessions')\n",
    "    ax.set_title('Channel Count Distribution')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No channel data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "overview_plot_path = OUTPUT_DIRS['plots'] / 'dataset_overview.png'\n",
    "plt.savefig(overview_plot_path, dpi=VIZ_PARAMS['figure_dpi'], bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved overview plot to: {overview_plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edab03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Core Pipeline (Test on Sample Subjects)\n",
    "\n",
    "This phase implements the core processing functions. Test on 5-10 subjects before running on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e04841",
   "metadata": {},
   "source": [
    "## Step 4: Load & Validate\n",
    "\n",
    "Functions to load EEG data and check data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e86c891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ load_eeg_data() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD EEG DATA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def load_eeg_data(subject_id, session_id, preload=True):\n",
    "    \"\"\"\n",
    "    Load EEG data for a specific subject and session.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    subject_id : str\n",
    "        Subject identifier (e.g., 'sub-NORB00001')\n",
    "    session_id : str\n",
    "        Session identifier (e.g., 'ses-1')\n",
    "    preload : bool\n",
    "        Whether to preload data into memory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    raw : mne.io.Raw or None\n",
    "        Raw EEG data object\n",
    "    metadata : dict\n",
    "        Dictionary containing metadata about the recording\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        'subject_id': subject_id,\n",
    "        'session_id': session_id,\n",
    "        'loaded': False,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Construct file path\n",
    "        eeg_file = DATASET_BASE_PATH / subject_id / session_id / 'eeg' / f\"{subject_id}_{session_id}_task-EEG_eeg.edf\"\n",
    "        \n",
    "        if not eeg_file.exists():\n",
    "            metadata['error'] = f\"File not found: {eeg_file}\"\n",
    "            logger.warning(metadata['error'])\n",
    "            return None, metadata\n",
    "        \n",
    "        # Load EEG data\n",
    "        raw = mne.io.read_raw_edf(eeg_file, preload=preload, verbose='ERROR')\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata['file_path'] = str(eeg_file)\n",
    "        metadata['duration_sec'] = raw.times[-1]\n",
    "        metadata['n_channels'] = len(raw.ch_names)\n",
    "        metadata['channel_names'] = raw.ch_names\n",
    "        metadata['sampling_freq'] = raw.info['sfreq']\n",
    "        metadata['loaded'] = True\n",
    "        \n",
    "        logger.info(f\"Loaded {subject_id}/{session_id}: {metadata['n_channels']} channels, \"\n",
    "                   f\"{metadata['duration_sec']:.1f}s @ {metadata['sampling_freq']}Hz\")\n",
    "        \n",
    "        return raw, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        metadata['error'] = str(e)\n",
    "        logger.error(f\"Error loading {subject_id}/{session_id}: {e}\")\n",
    "        return None, metadata\n",
    "\n",
    "\n",
    "print(\"✓ load_eeg_data() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab36963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ validate_eeg_data() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VALIDATE EEG DATA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_eeg_data(raw):\n",
    "    \"\"\"\n",
    "    Validate EEG data quality and identify potential issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw EEG data object\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    validation_report : dict\n",
    "        Dictionary containing validation results and identified issues\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_report = {\n",
    "        'is_valid': True,\n",
    "        'issues': [],\n",
    "        'warnings': [],\n",
    "        'bad_channels': [],\n",
    "        'sampling_rate': raw.info['sfreq'],\n",
    "        'n_channels': len(raw.ch_names),\n",
    "        'duration_sec': raw.times[-1]\n",
    "    }\n",
    "    \n",
    "    # Check 1: Verify sampling rate consistency\n",
    "    expected_sfreq = 200.0  # Hz\n",
    "    if abs(raw.info['sfreq'] - expected_sfreq) > 0.1:\n",
    "        validation_report['warnings'].append(\n",
    "            f\"Sampling rate {raw.info['sfreq']} Hz differs from expected {expected_sfreq} Hz\"\n",
    "        )\n",
    "    \n",
    "    # Check 2: Verify minimum number of channels\n",
    "    min_channels = 10\n",
    "    if len(raw.ch_names) < min_channels:\n",
    "        validation_report['issues'].append(\n",
    "            f\"Only {len(raw.ch_names)} channels (minimum {min_channels} expected)\"\n",
    "        )\n",
    "        validation_report['is_valid'] = False\n",
    "    \n",
    "    # Check 3: Check for flat channels\n",
    "    data = raw.get_data()\n",
    "    flat_threshold = 1e-10  # Very small value\n",
    "    \n",
    "    for ch_idx, ch_name in enumerate(raw.ch_names):\n",
    "        ch_data = data[ch_idx, :]\n",
    "        \n",
    "        # Check if channel is flat (no variation)\n",
    "        if np.std(ch_data) < flat_threshold:\n",
    "            validation_report['bad_channels'].append(ch_name)\n",
    "            validation_report['warnings'].append(f\"Channel {ch_name} appears flat (std < {flat_threshold})\")\n",
    "        \n",
    "        # Check for extreme values\n",
    "        if np.any(np.abs(ch_data) > 1e4):  # Very large amplitude (> 10,000 µV)\n",
    "            validation_report['warnings'].append(f\"Channel {ch_name} has extreme amplitude values\")\n",
    "        \n",
    "        # Check for NaN or Inf\n",
    "        if np.any(np.isnan(ch_data)) or np.any(np.isinf(ch_data)):\n",
    "            validation_report['bad_channels'].append(ch_name)\n",
    "            validation_report['issues'].append(f\"Channel {ch_name} contains NaN or Inf values\")\n",
    "            validation_report['is_valid'] = False\n",
    "    \n",
    "    # Check 4: Verify minimum duration\n",
    "    min_duration = 30.0  # seconds\n",
    "    if raw.times[-1] < min_duration:\n",
    "        validation_report['issues'].append(\n",
    "            f\"Recording duration {raw.times[-1]:.1f}s is less than minimum {min_duration}s\"\n",
    "        )\n",
    "        validation_report['is_valid'] = False\n",
    "    \n",
    "    # Summary\n",
    "    validation_report['n_bad_channels'] = len(validation_report['bad_channels'])\n",
    "    validation_report['bad_channel_fraction'] = len(validation_report['bad_channels']) / len(raw.ch_names)\n",
    "    \n",
    "    if validation_report['bad_channel_fraction'] > PREPROCESS_PARAMS['bad_channel_threshold']:\n",
    "        validation_report['issues'].append(\n",
    "            f\"Too many bad channels: {validation_report['n_bad_channels']}/{len(raw.ch_names)} \"\n",
    "            f\"({validation_report['bad_channel_fraction']*100:.1f}%)\"\n",
    "        )\n",
    "        validation_report['is_valid'] = False\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "\n",
    "print(\"✓ validate_eeg_data() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0473bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ load_events_and_annotations() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD EVENTS AND ANNOTATIONS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def load_events_and_annotations(subject_id, session_id):\n",
    "    \"\"\"\n",
    "    Load events and annotations for a session.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    subject_id : str\n",
    "        Subject identifier\n",
    "    session_id : str\n",
    "        Session identifier\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    events_df : pd.DataFrame or None\n",
    "        Events data (eyes open/closed markers)\n",
    "    annotations_df : pd.DataFrame or None\n",
    "        Derivative annotations (clean segments)\n",
    "    \"\"\"\n",
    "    \n",
    "    events_df = None\n",
    "    annotations_df = None\n",
    "    \n",
    "    # Load events file (eyes open/closed)\n",
    "    events_file = (DATASET_BASE_PATH / subject_id / session_id / 'eeg' / \n",
    "                   f\"{subject_id}_{session_id}_task-EEG_events.tsv\")\n",
    "    \n",
    "    if events_file.exists():\n",
    "        try:\n",
    "            events_df = pd.read_csv(events_file, sep='\\t')\n",
    "            logger.info(f\"Loaded events for {subject_id}/{session_id}: {len(events_df)} events\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load events file {events_file}: {e}\")\n",
    "    \n",
    "    # Load derivative annotations (clean segments)\n",
    "    annot_file = (DERIVATIVES_PATH / subject_id / session_id / 'eeg' / \n",
    "                  f\"{subject_id}_{session_id}_task-EEG_annotations.tsv\")\n",
    "    \n",
    "    if annot_file.exists():\n",
    "        try:\n",
    "            # Read with explicit delimiter and handle potential parsing issues\n",
    "            annotations_df = pd.read_csv(annot_file, sep='\\t', dtype={'onset': float, 'duration': float, 'label': str})\n",
    "            logger.info(f\"Loaded annotations for {subject_id}/{session_id}: {len(annotations_df)} annotations\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load annotations file {annot_file}: {e}\")\n",
    "    \n",
    "    return events_df, annotations_df\n",
    "\n",
    "\n",
    "print(\"✓ load_events_and_annotations() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19209884",
   "metadata": {},
   "source": [
    "### Test Load & Validate Functions\n",
    "\n",
    "Test the functions on a sample subject to verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c74e8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:01:46,499 - INFO - Loaded sub-NORB00001/ses-1: 21 channels, 714.0s @ 200.0Hz\n",
      "2025-10-25 17:01:46,509 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:01:46,509 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:01:46,510 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:01:46,510 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing load and validate functions on a sample subject...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Testing: sub-NORB00001/ses-1\n",
      "================================================================================\n",
      "\n",
      "1. Loading EEG data...\n",
      "   ✓ Loaded successfully\n",
      "   - Duration: 714.00 seconds\n",
      "   - Channels: 21\n",
      "   - Sampling rate: 200.0 Hz\n",
      "   - Channel names: Fp1, Fp2, F3, F4, C3...\n",
      "\n",
      "2. Validating data quality...\n",
      "   ✓ Data passed validation\n",
      "   - No bad channels detected\n",
      "\n",
      "3. Loading events and annotations...\n",
      "   ✓ Events loaded: 3 events\n",
      "     - discontinuity: 1\n",
      "     - eyes_closed: 1\n",
      "     - eyes_open: 1\n",
      "   ⚠ No annotations file found\n",
      "\n",
      "4. Creating quick visualization...\n",
      "   ✓ Saved plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_sample_raw.png\n",
      "\n",
      "================================================================================\n",
      "Completed testing sub-NORB00001/ses-1\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LOAD & VALIDATE TESTING COMPLETE\n",
      "================================================================================\n",
      "   ✓ Saved plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_sample_raw.png\n",
      "\n",
      "================================================================================\n",
      "Completed testing sub-NORB00001/ses-1\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LOAD & VALIDATE TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/382783452.py:118: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST ON SAMPLE SUBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Testing load and validate functions on a sample subject...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get first valid subject from inventory\n",
    "valid_subjects = inventory_df[inventory_df['file_exists']].head(3)\n",
    "\n",
    "if len(valid_subjects) == 0:\n",
    "    print(\"No valid subjects found in inventory!\")\n",
    "else:\n",
    "    for idx, row in valid_subjects.iterrows():\n",
    "        test_subject = row['subject_id']\n",
    "        test_session = row['session_id']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Testing: {test_subject}/{test_session}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Test load function\n",
    "        print(\"\\n1. Loading EEG data...\")\n",
    "        raw, metadata = load_eeg_data(test_subject, test_session, preload=True)\n",
    "        \n",
    "        if raw is None:\n",
    "            print(f\"   ✗ Failed to load: {metadata['error']}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"   ✓ Loaded successfully\")\n",
    "            print(f\"   - Duration: {metadata['duration_sec']:.2f} seconds\")\n",
    "            print(f\"   - Channels: {metadata['n_channels']}\")\n",
    "            print(f\"   - Sampling rate: {metadata['sampling_freq']} Hz\")\n",
    "            print(f\"   - Channel names: {', '.join(metadata['channel_names'][:5])}...\")\n",
    "        \n",
    "        # Test validation function\n",
    "        print(\"\\n2. Validating data quality...\")\n",
    "        validation = validate_eeg_data(raw)\n",
    "        \n",
    "        if validation['is_valid']:\n",
    "            print(\"   ✓ Data passed validation\")\n",
    "        else:\n",
    "            print(\"   ✗ Data failed validation\")\n",
    "        \n",
    "        if validation['bad_channels']:\n",
    "            print(f\"   - Bad channels: {', '.join(validation['bad_channels'])}\")\n",
    "        else:\n",
    "            print(\"   - No bad channels detected\")\n",
    "        \n",
    "        if validation['warnings']:\n",
    "            print(f\"   - Warnings ({len(validation['warnings'])}):\")\n",
    "            for warning in validation['warnings'][:3]:  # Show first 3\n",
    "                print(f\"     • {warning}\")\n",
    "        \n",
    "        if validation['issues']:\n",
    "            print(f\"   - Issues ({len(validation['issues'])}):\")\n",
    "            for issue in validation['issues']:\n",
    "                print(f\"     • {issue}\")\n",
    "        \n",
    "        # Test events and annotations\n",
    "        print(\"\\n3. Loading events and annotations...\")\n",
    "        events_df, annotations_df = load_events_and_annotations(test_subject, test_session)\n",
    "        \n",
    "        if events_df is not None:\n",
    "            print(f\"   ✓ Events loaded: {len(events_df)} events\")\n",
    "            if 'trial_type' in events_df.columns:\n",
    "                event_types = events_df['trial_type'].value_counts()\n",
    "                for event_type, count in event_types.items():\n",
    "                    print(f\"     - {event_type}: {count}\")\n",
    "        else:\n",
    "            print(\"   ⚠ No events file found\")\n",
    "        \n",
    "        if annotations_df is not None:\n",
    "            print(f\"   ✓ Annotations loaded: {len(annotations_df)} annotations\")\n",
    "            if 'label' in annotations_df.columns:\n",
    "                annot_types = annotations_df['label'].value_counts()\n",
    "                for annot_type, count in annot_types.items():\n",
    "                    print(f\"     - {annot_type}: {count}\")\n",
    "        else:\n",
    "            print(\"   ⚠ No annotations file found\")\n",
    "        \n",
    "        # Visualize raw data snippet\n",
    "        print(\"\\n4. Creating quick visualization...\")\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "            \n",
    "            # Plot 10 seconds of data\n",
    "            duration_to_plot = min(10, raw.times[-1])\n",
    "            \n",
    "            # Raw data\n",
    "            data_snippet, times_snippet = raw[:5, :int(duration_to_plot * raw.info['sfreq'])]\n",
    "            for i, ch_name in enumerate(raw.ch_names[:5]):\n",
    "                axes[0].plot(times_snippet, data_snippet[i, :] * 1e6 + i * 100, label=ch_name, alpha=0.7)\n",
    "            axes[0].set_xlabel('Time (s)')\n",
    "            axes[0].set_ylabel('Amplitude (µV) - Offset for visualization')\n",
    "            axes[0].set_title(f'Raw EEG Data - {test_subject}/{test_session} (first 5 channels)')\n",
    "            axes[0].legend(loc='upper right')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Power Spectral Density\n",
    "            psd_data = raw.compute_psd(fmin=0.5, fmax=50, n_fft=int(2*raw.info['sfreq']))\n",
    "            psds, freqs = psd_data.get_data(return_freqs=True)\n",
    "            \n",
    "            axes[1].semilogy(freqs, psds[:5, :].T, alpha=0.7)\n",
    "            axes[1].set_xlabel('Frequency (Hz)')\n",
    "            axes[1].set_ylabel('PSD (µV²/Hz)')\n",
    "            axes[1].set_title('Power Spectral Density (first 5 channels)')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].set_xlim([0, 50])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            sample_plot_path = OUTPUT_DIRS['plots'] / f'{test_subject}_{test_session}_sample_raw.png'\n",
    "            plt.savefig(sample_plot_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"   ✓ Saved plot to: {sample_plot_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ Could not create visualization: {e}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del raw\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Completed testing {test_subject}/{test_session}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Only test first valid subject in detail, show summary for others\n",
    "        if idx == valid_subjects.index[0]:\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOAD & VALIDATE TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d2ace",
   "metadata": {},
   "source": [
    "## Step 5: Preprocessing\n",
    "\n",
    "Functions to filter, re-reference, segment, and check stationarity of EEG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fe0b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ preprocess_eeg() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESS EEG FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_eeg(raw, params=None):\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to EEG data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw EEG data object\n",
    "    params : dict, optional\n",
    "        Preprocessing parameters (uses PREPROCESS_PARAMS if None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    raw_preprocessed : mne.io.Raw\n",
    "        Preprocessed EEG data\n",
    "    preprocess_info : dict\n",
    "        Information about preprocessing steps applied\n",
    "    \"\"\"\n",
    "    \n",
    "    if params is None:\n",
    "        params = PREPROCESS_PARAMS\n",
    "    \n",
    "    preprocess_info = {\n",
    "        'success': False,\n",
    "        'steps_applied': [],\n",
    "        'bad_channels_original': [],\n",
    "        'bad_channels_interpolated': [],\n",
    "        'reference': None,\n",
    "        'filters_applied': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make a copy to avoid modifying original\n",
    "        raw_prep = raw.copy()\n",
    "        \n",
    "        # Step 1: Identify bad channels from validation\n",
    "        if params['interpolate_bad']:\n",
    "            # Simple bad channel detection based on extreme values\n",
    "            data = raw_prep.get_data()\n",
    "            bad_channels = []\n",
    "            \n",
    "            for ch_idx, ch_name in enumerate(raw_prep.ch_names):\n",
    "                ch_data = data[ch_idx, :]\n",
    "                # Check for flat or extreme channels\n",
    "                if np.std(ch_data) < 1e-10 or np.any(np.abs(ch_data) > 1e4):\n",
    "                    bad_channels.append(ch_name)\n",
    "            \n",
    "            preprocess_info['bad_channels_original'] = bad_channels\n",
    "            \n",
    "            if bad_channels:\n",
    "                raw_prep.info['bads'] = bad_channels\n",
    "                logger.info(f\"Marked {len(bad_channels)} bad channels: {', '.join(bad_channels)}\")\n",
    "                preprocess_info['steps_applied'].append(f'marked_{len(bad_channels)}_bad_channels')\n",
    "        \n",
    "        # Step 2: Apply bandpass filter\n",
    "        logger.info(f\"Applying bandpass filter: {params['highpass_freq']}-{params['lowpass_freq']} Hz\")\n",
    "        raw_prep.filter(\n",
    "            l_freq=params['highpass_freq'],\n",
    "            h_freq=params['lowpass_freq'],\n",
    "            method=params['filter_method'],\n",
    "            verbose='ERROR'\n",
    "        )\n",
    "        preprocess_info['filters_applied'].append(f\"bandpass_{params['highpass_freq']}-{params['lowpass_freq']}_Hz\")\n",
    "        preprocess_info['steps_applied'].append('bandpass_filter')\n",
    "        \n",
    "        # Step 3: Apply notch filter\n",
    "        logger.info(f\"Applying notch filter at {params['notch_freq']} Hz\")\n",
    "        raw_prep.notch_filter(\n",
    "            freqs=params['notch_freq'],\n",
    "            verbose='ERROR'\n",
    "        )\n",
    "        preprocess_info['filters_applied'].append(f\"notch_{params['notch_freq']}_Hz\")\n",
    "        preprocess_info['steps_applied'].append('notch_filter')\n",
    "        \n",
    "        # Step 4: Interpolate bad channels\n",
    "        if params['interpolate_bad'] and len(preprocess_info['bad_channels_original']) > 0:\n",
    "            logger.info(f\"Interpolating {len(preprocess_info['bad_channels_original'])} bad channels\")\n",
    "            raw_prep.interpolate_bads(reset_bads=True, verbose='ERROR')\n",
    "            preprocess_info['bad_channels_interpolated'] = preprocess_info['bad_channels_original'].copy()\n",
    "            preprocess_info['steps_applied'].append(f'interpolated_{len(preprocess_info[\"bad_channels_interpolated\"])}_channels')\n",
    "        \n",
    "        # Step 5: Re-reference\n",
    "        if params['re_reference']:\n",
    "            ref_type = params['reference']\n",
    "            logger.info(f\"Re-referencing to: {ref_type}\")\n",
    "            \n",
    "            if ref_type == 'average':\n",
    "                raw_prep.set_eeg_reference('average', projection=False, verbose='ERROR')\n",
    "            elif ref_type == 'common':\n",
    "                # Already common reference, no change needed\n",
    "                pass\n",
    "            else:\n",
    "                # Specific channel(s) as reference\n",
    "                raw_prep.set_eeg_reference(ref_type, projection=False, verbose='ERROR')\n",
    "            \n",
    "            preprocess_info['reference'] = ref_type\n",
    "            preprocess_info['steps_applied'].append(f'reference_{ref_type}')\n",
    "        \n",
    "        preprocess_info['success'] = True\n",
    "        logger.info(f\"Preprocessing completed successfully: {', '.join(preprocess_info['steps_applied'])}\")\n",
    "        \n",
    "        return raw_prep, preprocess_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during preprocessing: {e}\")\n",
    "        preprocess_info['error'] = str(e)\n",
    "        return None, preprocess_info\n",
    "\n",
    "\n",
    "print(\"✓ preprocess_eeg() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "daef8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ segment_data() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SEGMENT DATA FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def segment_data(raw, events_df=None, annotations_df=None, params=None):\n",
    "    \"\"\"\n",
    "    Segment EEG data into analysis windows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw : mne.io.Raw\n",
    "        Preprocessed EEG data\n",
    "    events_df : pd.DataFrame, optional\n",
    "        Events data (eyes open/closed markers)\n",
    "    annotations_df : pd.DataFrame, optional\n",
    "        Derivative annotations (clean segments)\n",
    "    params : dict, optional\n",
    "        Segmentation parameters (uses SEGMENT_PARAMS if None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    segments : list of np.ndarray\n",
    "        List of data segments (n_channels x n_timepoints)\n",
    "    segment_info : list of dict\n",
    "        Information about each segment (start_time, duration, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if params is None:\n",
    "        params = SEGMENT_PARAMS\n",
    "    \n",
    "    segments = []\n",
    "    segment_info = []\n",
    "    \n",
    "    try:\n",
    "        # Get sampling frequency\n",
    "        sfreq = raw.info['sfreq']\n",
    "        window_samples = int(params['window_length'] * sfreq)\n",
    "        overlap_samples = int(params['window_length'] * params['window_overlap'] * sfreq)\n",
    "        step_samples = window_samples - overlap_samples\n",
    "        \n",
    "        # Determine valid time ranges based on annotations\n",
    "        valid_ranges = []\n",
    "        \n",
    "        if params['use_eyes_closed_only'] and events_df is not None:\n",
    "            # Extract eyes-closed periods\n",
    "            eyes_closed_events = events_df[events_df['trial_type'] == 'eyes_closed'].copy()\n",
    "            \n",
    "            # Sort by onset time to properly find next event\n",
    "            events_sorted = events_df.sort_values('onset').reset_index(drop=True)\n",
    "            \n",
    "            for _, event in eyes_closed_events.iterrows():\n",
    "                start_time = event['onset']\n",
    "                \n",
    "                # Find the end time by looking for the next event (eyes_open or end of recording)\n",
    "                # Get events that occur after this one\n",
    "                future_events = events_sorted[events_sorted['onset'] > start_time]\n",
    "                \n",
    "                if len(future_events) > 0:\n",
    "                    # Use the next event as end time\n",
    "                    end_time = future_events.iloc[0]['onset']\n",
    "                else:\n",
    "                    # Use end of recording\n",
    "                    end_time = raw.times[-1]\n",
    "                \n",
    "                # Check if duration meets minimum requirement\n",
    "                if end_time - start_time >= params['min_segment_duration']:\n",
    "                    valid_ranges.append((start_time, end_time))\n",
    "        \n",
    "        # If reject_artifacts is True and we have derivative annotations\n",
    "        if params['reject_artifacts'] and annotations_df is not None:\n",
    "            # Use derivative annotations as valid ranges\n",
    "            clean_ranges = []\n",
    "            \n",
    "            for _, annot in annotations_df.iterrows():\n",
    "                try:\n",
    "                    start_time = float(annot['onset'])\n",
    "                    duration = float(annot['duration'])\n",
    "                    \n",
    "                    if duration >= params['min_segment_duration']:\n",
    "                        end_time = start_time + duration\n",
    "                        clean_ranges.append((start_time, end_time))\n",
    "                except (ValueError, TypeError, KeyError) as e:\n",
    "                    logger.warning(f\"Could not parse annotation row: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Intersect with eyes-closed ranges if applicable\n",
    "            if valid_ranges:\n",
    "                intersected = []\n",
    "                for ec_start, ec_end in valid_ranges:\n",
    "                    for clean_start, clean_end in clean_ranges:\n",
    "                        overlap_start = max(ec_start, clean_start)\n",
    "                        overlap_end = min(ec_end, clean_end)\n",
    "                        if overlap_end - overlap_start >= params['min_segment_duration']:\n",
    "                            intersected.append((overlap_start, overlap_end))\n",
    "                valid_ranges = intersected\n",
    "            else:\n",
    "                valid_ranges = clean_ranges\n",
    "        \n",
    "        # If no valid ranges defined, use entire recording\n",
    "        if not valid_ranges:\n",
    "            valid_ranges = [(0, raw.times[-1])]\n",
    "            logger.info(\"No specific valid ranges found, using entire recording\")\n",
    "        \n",
    "        # Extract segments from valid ranges\n",
    "        for range_start, range_end in valid_ranges:\n",
    "            start_sample = int(range_start * sfreq)\n",
    "            end_sample = int(range_end * sfreq)\n",
    "            \n",
    "            # Create sliding windows within this range\n",
    "            current_sample = start_sample\n",
    "            while current_sample + window_samples <= end_sample:\n",
    "                # Extract segment\n",
    "                segment_data = raw[:, current_sample:current_sample + window_samples][0]\n",
    "                \n",
    "                # Store segment\n",
    "                segments.append(segment_data)\n",
    "                \n",
    "                # Store info\n",
    "                info = {\n",
    "                    'start_time': current_sample / sfreq,\n",
    "                    'end_time': (current_sample + window_samples) / sfreq,\n",
    "                    'duration': params['window_length'],\n",
    "                    'n_samples': window_samples,\n",
    "                    'segment_index': len(segments) - 1\n",
    "                }\n",
    "                segment_info.append(info)\n",
    "                \n",
    "                # Move to next window\n",
    "                current_sample += step_samples\n",
    "        \n",
    "        logger.info(f\"Extracted {len(segments)} segments \"\n",
    "                   f\"(window: {params['window_length']}s, overlap: {params['window_overlap']*100:.0f}%)\")\n",
    "        \n",
    "        return segments, segment_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during segmentation: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "print(\"✓ segment_data() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78fd21dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ check_stationarity() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK STATIONARITY FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def check_stationarity(segment_data, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Check stationarity of time series data using Augmented Dickey-Fuller test.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_data : np.ndarray\n",
    "        Data segment (n_channels x n_timepoints)\n",
    "    significance_level : float\n",
    "        Significance level for ADF test (default: 0.05)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    stationarity_report : dict\n",
    "        Dictionary with stationarity test results per channel\n",
    "    \"\"\"\n",
    "    \n",
    "    n_channels = segment_data.shape[0]\n",
    "    \n",
    "    stationarity_report = {\n",
    "        'is_stationary': [],\n",
    "        'adf_statistics': [],\n",
    "        'p_values': [],\n",
    "        'n_stationary': 0,\n",
    "        'n_non_stationary': 0,\n",
    "        'fraction_stationary': 0.0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for ch_idx in range(n_channels):\n",
    "            ch_data = segment_data[ch_idx, :]\n",
    "            \n",
    "            # Perform ADF test\n",
    "            adf_result = adfuller(ch_data, autolag='AIC')\n",
    "            adf_statistic = adf_result[0]\n",
    "            p_value = adf_result[1]\n",
    "            \n",
    "            # Stationary if we can reject null hypothesis (null = unit root / non-stationary)\n",
    "            is_stationary = p_value < significance_level\n",
    "            \n",
    "            stationarity_report['is_stationary'].append(is_stationary)\n",
    "            stationarity_report['adf_statistics'].append(adf_statistic)\n",
    "            stationarity_report['p_values'].append(p_value)\n",
    "        \n",
    "        # Summary statistics\n",
    "        stationarity_report['n_stationary'] = sum(stationarity_report['is_stationary'])\n",
    "        stationarity_report['n_non_stationary'] = n_channels - stationarity_report['n_stationary']\n",
    "        stationarity_report['fraction_stationary'] = stationarity_report['n_stationary'] / n_channels\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking stationarity: {e}\")\n",
    "        stationarity_report['error'] = str(e)\n",
    "    \n",
    "    return stationarity_report\n",
    "\n",
    "\n",
    "print(\"✓ check_stationarity() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f94032",
   "metadata": {},
   "source": [
    "### Test Preprocessing Functions\n",
    "\n",
    "Test preprocessing, segmentation, and stationarity checking on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ef62597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:14:47,648 - INFO - Loaded sub-NORB00001/ses-1: 21 channels, 714.0s @ 200.0Hz\n",
      "2025-10-25 17:14:47,670 - INFO - Applying bandpass filter: 0.5-30.0 Hz\n",
      "2025-10-25 17:14:47,670 - INFO - Applying bandpass filter: 0.5-30.0 Hz\n",
      "2025-10-25 17:14:47,717 - INFO - Applying notch filter at 60.0 Hz\n",
      "2025-10-25 17:14:47,717 - INFO - Applying notch filter at 60.0 Hz\n",
      "2025-10-25 17:14:47,760 - INFO - Re-referencing to: average\n",
      "2025-10-25 17:14:47,760 - INFO - Re-referencing to: average\n",
      "2025-10-25 17:14:47,776 - INFO - Preprocessing completed successfully: bandpass_filter, notch_filter, reference_average\n",
      "2025-10-25 17:14:47,779 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:14:47,780 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:14:47,776 - INFO - Preprocessing completed successfully: bandpass_filter, notch_filter, reference_average\n",
      "2025-10-25 17:14:47,779 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:14:47,780 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:14:47,790 - INFO - Extracted 127 segments (window: 10.0s, overlap: 50%)\n",
      "2025-10-25 17:14:47,790 - INFO - Extracted 127 segments (window: 10.0s, overlap: 50%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing functions on sample subject...\n",
      "================================================================================\n",
      "\n",
      "Processing: sub-NORB00001/ses-1\n",
      "================================================================================\n",
      "\n",
      "1. Loading EEG data...\n",
      "✓ Loaded: 21 channels, 714.0s\n",
      "\n",
      "2. Preprocessing...\n",
      "✓ Preprocessing completed\n",
      "   Steps applied: bandpass_filter, notch_filter, reference_average\n",
      "   Filters: bandpass_0.5-30.0_Hz, notch_60.0_Hz\n",
      "   Reference: average\n",
      "\n",
      "3. Loading annotations...\n",
      "✓ Events: 3 events\n",
      "\n",
      "4. Segmenting data...\n",
      "✓ Extracted 127 segments\n",
      "   Window length: 10.0s\n",
      "   Window overlap: 50%\n",
      "   Segment shape: (21, 2000) (channels x samples)\n",
      "\n",
      "5. Checking stationarity (first 3 segments)...\n",
      "   Segment 1:\n",
      "     - Stationary channels: 19/21 (90.5%)\n",
      "     - Mean p-value: 0.0153\n",
      "   Segment 1:\n",
      "     - Stationary channels: 19/21 (90.5%)\n",
      "     - Mean p-value: 0.0153\n",
      "   Segment 2:\n",
      "     - Stationary channels: 21/21 (100.0%)\n",
      "     - Mean p-value: 0.0029\n",
      "   Segment 2:\n",
      "     - Stationary channels: 21/21 (100.0%)\n",
      "     - Mean p-value: 0.0029\n",
      "   Segment 3:\n",
      "     - Stationary channels: 19/21 (90.5%)\n",
      "     - Mean p-value: 0.0053\n",
      "\n",
      "6. Creating preprocessing comparison plots...\n",
      "   Segment 3:\n",
      "     - Stationary channels: 19/21 (90.5%)\n",
      "     - Mean p-value: 0.0053\n",
      "\n",
      "6. Creating preprocessing comparison plots...\n",
      "✓ Saved preprocessing plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_preprocessing.png\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING TESTING COMPLETE\n",
      "================================================================================\n",
      "✓ Saved preprocessing plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_preprocessing.png\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/2437210739.py:168: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST PREPROCESSING ON SAMPLE SUBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Testing preprocessing functions on sample subject...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get first valid subject\n",
    "valid_subject = inventory_df[inventory_df['file_exists']].iloc[0]\n",
    "test_subject = valid_subject['subject_id']\n",
    "test_session = valid_subject['session_id']\n",
    "\n",
    "print(f\"\\nProcessing: {test_subject}/{test_session}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Load data\n",
    "print(\"\\n1. Loading EEG data...\")\n",
    "raw, metadata = load_eeg_data(test_subject, test_session, preload=True)\n",
    "\n",
    "if raw is None:\n",
    "    print(f\"✗ Failed to load data\")\n",
    "else:\n",
    "    print(f\"✓ Loaded: {metadata['n_channels']} channels, {metadata['duration_sec']:.1f}s\")\n",
    "    \n",
    "    # Step 2: Preprocess\n",
    "    print(\"\\n2. Preprocessing...\")\n",
    "    raw_prep, preprocess_info = preprocess_eeg(raw)\n",
    "    \n",
    "    if raw_prep is None:\n",
    "        print(f\"✗ Preprocessing failed: {preprocess_info.get('error', 'Unknown error')}\")\n",
    "    else:\n",
    "        print(f\"✓ Preprocessing completed\")\n",
    "        print(f\"   Steps applied: {', '.join(preprocess_info['steps_applied'])}\")\n",
    "        if preprocess_info['bad_channels_original']:\n",
    "            print(f\"   Bad channels: {', '.join(preprocess_info['bad_channels_original'])}\")\n",
    "        print(f\"   Filters: {', '.join(preprocess_info['filters_applied'])}\")\n",
    "        print(f\"   Reference: {preprocess_info['reference']}\")\n",
    "    \n",
    "    # Step 3: Load annotations\n",
    "    print(\"\\n3. Loading annotations...\")\n",
    "    events_df, annotations_df = load_events_and_annotations(test_subject, test_session)\n",
    "    \n",
    "    if events_df is not None:\n",
    "        print(f\"✓ Events: {len(events_df)} events\")\n",
    "    if annotations_df is not None:\n",
    "        print(f\"✓ Annotations: {len(annotations_df)} annotations\")\n",
    "    \n",
    "    # Step 4: Segment data\n",
    "    print(\"\\n4. Segmenting data...\")\n",
    "    segments, segment_info = segment_data(raw_prep, events_df, annotations_df)\n",
    "    \n",
    "    if len(segments) == 0:\n",
    "        print(\"✗ No segments extracted\")\n",
    "    else:\n",
    "        print(f\"✓ Extracted {len(segments)} segments\")\n",
    "        print(f\"   Window length: {SEGMENT_PARAMS['window_length']}s\")\n",
    "        print(f\"   Window overlap: {SEGMENT_PARAMS['window_overlap']*100:.0f}%\")\n",
    "        print(f\"   Segment shape: {segments[0].shape} (channels x samples)\")\n",
    "    \n",
    "    # Step 5: Check stationarity on first few segments\n",
    "    if len(segments) > 0:\n",
    "        print(\"\\n5. Checking stationarity (first 3 segments)...\")\n",
    "        \n",
    "        for i in range(min(3, len(segments))):\n",
    "            stationarity = check_stationarity(segments[i])\n",
    "            print(f\"   Segment {i+1}:\")\n",
    "            print(f\"     - Stationary channels: {stationarity['n_stationary']}/{len(stationarity['is_stationary'])} \"\n",
    "                  f\"({stationarity['fraction_stationary']*100:.1f}%)\")\n",
    "            print(f\"     - Mean p-value: {np.mean(stationarity['p_values']):.4f}\")\n",
    "    \n",
    "    # Step 6: Visualize preprocessing effects\n",
    "    print(\"\\n6. Creating preprocessing comparison plots...\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "        fig.suptitle(f'Preprocessing Effects - {test_subject}/{test_session}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Get 10 seconds of data for visualization\n",
    "        duration_plot = min(10, raw.times[-1])\n",
    "        n_samples_plot = int(duration_plot * raw.info['sfreq'])\n",
    "        \n",
    "        # Plot 1: Raw data (time domain)\n",
    "        ax = axes[0, 0]\n",
    "        data_raw, times = raw[:5, :n_samples_plot]\n",
    "        for i in range(5):\n",
    "            ax.plot(times, data_raw[i, :] * 1e6 + i * 50, alpha=0.7, label=raw.ch_names[i])\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude (µV)')\n",
    "        ax.set_title('Original Raw Data (first 5 channels)')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Preprocessed data (time domain)\n",
    "        ax = axes[0, 1]\n",
    "        data_prep, times = raw_prep[:5, :n_samples_plot]\n",
    "        for i in range(5):\n",
    "            ax.plot(times, data_prep[i, :] * 1e6 + i * 50, alpha=0.7, label=raw_prep.ch_names[i])\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude (µV)')\n",
    "        ax.set_title('Preprocessed Data (first 5 channels)')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Raw PSD\n",
    "        ax = axes[1, 0]\n",
    "        psd_raw = raw.compute_psd(fmin=0.5, fmax=50, n_fft=int(2*raw.info['sfreq']))\n",
    "        psds_raw, freqs_raw = psd_raw.get_data(return_freqs=True)\n",
    "        ax.semilogy(freqs_raw, psds_raw[:5, :].T, alpha=0.7)\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('PSD (µV²/Hz)')\n",
    "        ax.set_title('Original PSD')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, 50])\n",
    "        \n",
    "        # Plot 4: Preprocessed PSD\n",
    "        ax = axes[1, 1]\n",
    "        psd_prep = raw_prep.compute_psd(fmin=0.5, fmax=50, n_fft=int(2*raw_prep.info['sfreq']))\n",
    "        psds_prep, freqs_prep = psd_prep.get_data(return_freqs=True)\n",
    "        ax.semilogy(freqs_prep, psds_prep[:5, :].T, alpha=0.7)\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('PSD (µV²/Hz)')\n",
    "        ax.set_title('Preprocessed PSD (filtered)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, 50])\n",
    "        \n",
    "        # Plot 5: Segment distribution\n",
    "        ax = axes[2, 0]\n",
    "        if len(segment_info) > 0:\n",
    "            start_times = [info['start_time'] for info in segment_info]\n",
    "            ax.hist(start_times, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            ax.set_ylabel('Number of Segments')\n",
    "            ax.set_title(f'Segment Distribution (n={len(segments)})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No segments', ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # Plot 6: Stationarity summary\n",
    "        ax = axes[2, 1]\n",
    "        if len(segments) > 0:\n",
    "            # Check stationarity for first 10 segments\n",
    "            n_check = min(10, len(segments))\n",
    "            stationary_fractions = []\n",
    "            for i in range(n_check):\n",
    "                stat = check_stationarity(segments[i])\n",
    "                stationary_fractions.append(stat['fraction_stationary'] * 100)\n",
    "            \n",
    "            ax.bar(range(1, n_check+1), stationary_fractions, color='mediumseagreen', \n",
    "                   edgecolor='black', alpha=0.7)\n",
    "            ax.axhline(y=80, color='red', linestyle='--', label='80% threshold')\n",
    "            ax.set_xlabel('Segment Number')\n",
    "            ax.set_ylabel('Stationary Channels (%)')\n",
    "            ax.set_title(f'Stationarity Check (first {n_check} segments)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim([0, 100])\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No segments', ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        preprocess_plot_path = OUTPUT_DIRS['plots'] / f'{test_subject}_{test_session}_preprocessing.png'\n",
    "        plt.savefig(preprocess_plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Saved preprocessing plot to: {preprocess_plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not create visualization: {e}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del raw, raw_prep, segments\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f0c20",
   "metadata": {},
   "source": [
    "## Step 6: Model Order Selection\n",
    "\n",
    "Functions to select optimal VAR model order using information criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cc65d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ select_model_order() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SELECT MODEL ORDER FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def select_model_order(segment_data, params=None):\n",
    "    \"\"\"\n",
    "    Select optimal VAR model order using information criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_data : np.ndarray\n",
    "        Data segment (n_channels x n_timepoints)\n",
    "    params : dict, optional\n",
    "        Model order parameters (uses GC_PARAMS if None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    order_info : dict\n",
    "        Dictionary containing optimal order and information criteria values\n",
    "    \"\"\"\n",
    "    \n",
    "    if params is None:\n",
    "        params = GC_PARAMS\n",
    "    \n",
    "    order_info = {\n",
    "        'success': False,\n",
    "        'optimal_order': None,\n",
    "        'method': params['model_order_method'],\n",
    "        'orders_tested': [],\n",
    "        'aic_values': [],\n",
    "        'bic_values': [],\n",
    "        'hqc_values': [],\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Transpose to (n_timepoints x n_channels) for statsmodels VAR\n",
    "        data_transposed = segment_data.T\n",
    "        \n",
    "        # If using fixed order, skip optimization\n",
    "        if params['model_order_method'] == 'fixed' and params['fixed_order'] is not None:\n",
    "            order_info['optimal_order'] = params['fixed_order']\n",
    "            order_info['success'] = True\n",
    "            logger.info(f\"Using fixed model order: {params['fixed_order']}\")\n",
    "            return order_info\n",
    "        \n",
    "        # Test different model orders\n",
    "        min_order = params['min_order']\n",
    "        max_order = min(params['max_order'], len(data_transposed) // 2)  # Limit based on data length\n",
    "        \n",
    "        aic_values = []\n",
    "        bic_values = []\n",
    "        hqc_values = []\n",
    "        orders_tested = []\n",
    "        \n",
    "        logger.info(f\"Testing VAR model orders from {min_order} to {max_order}...\")\n",
    "        \n",
    "        for order in range(min_order, max_order + 1):\n",
    "            try:\n",
    "                # Fit VAR model with current order\n",
    "                model = VAR(data_transposed)\n",
    "                results = model.fit(maxlags=order, ic=None, verbose=False)\n",
    "                \n",
    "                # Store information criteria values\n",
    "                aic_values.append(results.aic)\n",
    "                bic_values.append(results.bic)\n",
    "                hqc_values.append(results.hqic)\n",
    "                orders_tested.append(order)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not fit VAR model with order {order}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(orders_tested) == 0:\n",
    "            order_info['error'] = \"Could not fit any VAR models\"\n",
    "            logger.error(order_info['error'])\n",
    "            return order_info\n",
    "        \n",
    "        # Store results\n",
    "        order_info['orders_tested'] = orders_tested\n",
    "        order_info['aic_values'] = aic_values\n",
    "        order_info['bic_values'] = bic_values\n",
    "        order_info['hqc_values'] = hqc_values\n",
    "        \n",
    "        # Select optimal order based on chosen criterion\n",
    "        method = params['model_order_method'].lower()\n",
    "        \n",
    "        if method == 'aic':\n",
    "            optimal_idx = np.argmin(aic_values)\n",
    "            order_info['optimal_order'] = orders_tested[optimal_idx]\n",
    "            order_info['optimal_aic'] = aic_values[optimal_idx]\n",
    "        elif method == 'bic':\n",
    "            optimal_idx = np.argmin(bic_values)\n",
    "            order_info['optimal_order'] = orders_tested[optimal_idx]\n",
    "            order_info['optimal_bic'] = bic_values[optimal_idx]\n",
    "        elif method == 'hqc':\n",
    "            optimal_idx = np.argmin(hqc_values)\n",
    "            order_info['optimal_order'] = orders_tested[optimal_idx]\n",
    "            order_info['optimal_hqc'] = hqc_values[optimal_idx]\n",
    "        else:\n",
    "            # Default to AIC\n",
    "            optimal_idx = np.argmin(aic_values)\n",
    "            order_info['optimal_order'] = orders_tested[optimal_idx]\n",
    "            logger.warning(f\"Unknown method '{method}', defaulting to AIC\")\n",
    "        \n",
    "        order_info['success'] = True\n",
    "        logger.info(f\"Optimal model order selected: {order_info['optimal_order']} (method: {method})\")\n",
    "        \n",
    "        return order_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        order_info['error'] = str(e)\n",
    "        logger.error(f\"Error during model order selection: {e}\")\n",
    "        return order_info\n",
    "\n",
    "\n",
    "print(\"✓ select_model_order() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d35f1a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ validate_var_model() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VALIDATE VAR MODEL FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_var_model(segment_data, order, params=None):\n",
    "    \"\"\"\n",
    "    Validate a fitted VAR model by checking residuals and stability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_data : np.ndarray\n",
    "        Data segment (n_channels x n_timepoints)\n",
    "    order : int\n",
    "        Model order to validate\n",
    "    params : dict, optional\n",
    "        Statistical parameters (uses STAT_PARAMS if None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    validation_report : dict\n",
    "        Dictionary containing validation results\n",
    "    \"\"\"\n",
    "    \n",
    "    if params is None:\n",
    "        params = STAT_PARAMS\n",
    "    \n",
    "    validation_report = {\n",
    "        'is_valid': True,\n",
    "        'order': order,\n",
    "        'issues': [],\n",
    "        'warnings': [],\n",
    "        'residuals_white': None,\n",
    "        'model_stable': None,\n",
    "        'test_statistics': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Transpose to (n_timepoints x n_channels) for statsmodels VAR\n",
    "        data_transposed = segment_data.T\n",
    "        \n",
    "        # Fit VAR model\n",
    "        model = VAR(data_transposed)\n",
    "        results = model.fit(maxlags=order, ic=None, verbose=False)\n",
    "        \n",
    "        # Test 1: Check model stability (eigenvalues of companion matrix)\n",
    "        # Stable if all eigenvalues have modulus < 1\n",
    "        try:\n",
    "            is_stable = results.is_stable()\n",
    "            validation_report['model_stable'] = is_stable\n",
    "            \n",
    "            if not is_stable:\n",
    "                validation_report['issues'].append(f\"Model with order {order} is not stable\")\n",
    "                validation_report['is_valid'] = False\n",
    "                logger.warning(f\"VAR model with order {order} is unstable\")\n",
    "        except Exception as e:\n",
    "            validation_report['warnings'].append(f\"Could not check stability: {e}\")\n",
    "        \n",
    "        # Test 2: Residual whiteness (Portmanteau test)\n",
    "        # H0: residuals are white noise (no autocorrelation)\n",
    "        try:\n",
    "            from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "            \n",
    "            # Get residuals\n",
    "            residuals = results.resid\n",
    "            \n",
    "            # Test each channel's residuals\n",
    "            white_channels = []\n",
    "            ljungbox_pvalues = []\n",
    "            \n",
    "            for ch_idx in range(residuals.shape[1]):\n",
    "                # Ljung-Box test for autocorrelation\n",
    "                lb_test = acorr_ljungbox(residuals[:, ch_idx], lags=[10], return_df=False)\n",
    "                pvalue = lb_test[1][0]  # p-value\n",
    "                ljungbox_pvalues.append(pvalue)\n",
    "                \n",
    "                # Residuals are white if we cannot reject H0 (p > threshold)\n",
    "                is_white = pvalue > params['significance_threshold']\n",
    "                white_channels.append(is_white)\n",
    "            \n",
    "            validation_report['residuals_white'] = np.mean(white_channels)  # Fraction of white residuals\n",
    "            validation_report['test_statistics']['ljungbox_pvalues'] = ljungbox_pvalues\n",
    "            \n",
    "            if validation_report['residuals_white'] < 0.5:\n",
    "                validation_report['warnings'].append(\n",
    "                    f\"Only {validation_report['residuals_white']*100:.1f}% of channels have white residuals\"\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Residual whiteness: {validation_report['residuals_white']*100:.1f}% of channels\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            validation_report['warnings'].append(f\"Could not test residual whiteness: {e}\")\n",
    "        \n",
    "        # Test 3: Check for reasonable fit (R-squared)\n",
    "        try:\n",
    "            # Get R-squared values per equation\n",
    "            rsq_values = []\n",
    "            for eq_name, eq_results in results.summary().tables[1].items():\n",
    "                rsq = results.rsquared[eq_name] if hasattr(results, 'rsquared') else None\n",
    "                if rsq is not None:\n",
    "                    rsq_values.append(rsq)\n",
    "            \n",
    "            if rsq_values:\n",
    "                mean_rsq = np.mean(rsq_values)\n",
    "                validation_report['test_statistics']['mean_rsquared'] = mean_rsq\n",
    "                \n",
    "                if mean_rsq < 0.1:\n",
    "                    validation_report['warnings'].append(f\"Low R-squared: {mean_rsq:.3f}\")\n",
    "        except Exception as e:\n",
    "            # R-squared check is optional\n",
    "            pass\n",
    "        \n",
    "        logger.info(f\"VAR model validation complete for order {order}\")\n",
    "        \n",
    "        return validation_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_report['error'] = str(e)\n",
    "        validation_report['is_valid'] = False\n",
    "        logger.error(f\"Error during model validation: {e}\")\n",
    "        return validation_report\n",
    "\n",
    "\n",
    "print(\"✓ validate_var_model() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76915ffb",
   "metadata": {},
   "source": [
    "### Test Model Order Selection\n",
    "\n",
    "Test the model order selection on sample segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a358d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:09:56,961 - INFO - Loaded sub-NORB00001/ses-1: 21 channels, 714.0s @ 200.0Hz\n",
      "2025-10-25 17:09:56,979 - INFO - Applying bandpass filter: 0.5-30.0 Hz\n",
      "2025-10-25 17:09:57,023 - INFO - Applying notch filter at 60.0 Hz\n",
      "2025-10-25 17:09:57,065 - INFO - Re-referencing to: average\n",
      "2025-10-25 17:09:57,082 - INFO - Preprocessing completed successfully: bandpass_filter, notch_filter, reference_average\n",
      "2025-10-25 17:09:57,085 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:09:57,085 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:09:57,093 - INFO - Extracted 127 segments (window: 10.0s, overlap: 50%)\n",
      "2025-10-25 17:09:57,094 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:09:57,099 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,122 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model order selection on sample subject...\n",
      "================================================================================\n",
      "\n",
      "Processing: sub-NORB00001/ses-1\n",
      "================================================================================\n",
      "\n",
      "1. Loading and preprocessing EEG data...\n",
      "✓ Loaded: 21 channels, 714.0s\n",
      "✓ Preprocessing completed\n",
      "\n",
      "2. Segmenting data...\n",
      "✓ Extracted 127 segments\n",
      "\n",
      "3. Testing model order selection...\n",
      "   Method: AIC\n",
      "   Testing orders: 1 to 50\n",
      "\n",
      "   Segment 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:09:57,156 - WARNING - Could not fit VAR model with order 5: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,229 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,261 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,300 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,337 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,367 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,470 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,552 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,603 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,650 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,758 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,843 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:57,929 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:58,300 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:58,513 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:58,856 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:59,096 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:59,433 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:59,704 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:59,829 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:09:59,956 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:00,125 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:00,281 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:00,443 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:00,746 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:00,906 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:01,065 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:01,565 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:01,745 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:01,934 - INFO - Optimal model order selected: 50 (method: aic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Optimal order: 50\n",
      "     - Tested 19 orders\n",
      "     - AIC range: -844.77 to -618.52\n",
      "     - BIC range: -817.89 to -615.99\n",
      "     - Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alookaladdoo/.local/lib/python3.13/site-packages/statsmodels/tsa/vector_ar/var_model.py:1558: RuntimeWarning: invalid value encountered in sqrt\n",
      "  stderr = np.sqrt(np.diag(self.cov_params()))\n",
      "2025-10-25 17:10:10,604 - INFO - VAR model validation complete for order 50\n",
      "2025-10-25 17:10:10,606 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:10:10,622 - WARNING - Could not fit VAR model with order 2: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:10,630 - WARNING - Could not fit VAR model with order 3: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:10,676 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:10,703 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:10,729 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:10,757 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Model is valid\n",
      "       ✓ Stability: Stable\n",
      "       - Warnings: 1\n",
      "         • Could not test residual whiteness: 1\n",
      "\n",
      "   Segment 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:10:10,922 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,271 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,462 - WARNING - Could not fit VAR model with order 24: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,527 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,594 - WARNING - Could not fit VAR model with order 26: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,675 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,765 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:11,944 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:12,115 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:12,305 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:12,433 - WARNING - Could not fit VAR model with order 35: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:12,536 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:13,039 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:13,485 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:13,639 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:14,105 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:14,299 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:14,506 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:14,698 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:14,699 - INFO - Optimal model order selected: 42 (method: aic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Optimal order: 42\n",
      "     - Tested 25 orders\n",
      "     - AIC range: -846.43 to -584.74\n",
      "     - BIC range: -821.80 to -583.45\n",
      "     - Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alookaladdoo/.local/lib/python3.13/site-packages/statsmodels/tsa/vector_ar/var_model.py:1558: RuntimeWarning: invalid value encountered in sqrt\n",
      "  stderr = np.sqrt(np.diag(self.cov_params()))\n",
      "2025-10-25 17:10:17,872 - INFO - VAR model validation complete for order 42\n",
      "2025-10-25 17:10:17,873 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:10:17,882 - WARNING - Could not fit VAR model with order 2: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:17,945 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,022 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Model is valid\n",
      "       ✓ Stability: Stable\n",
      "       - Warnings: 1\n",
      "         • Could not test residual whiteness: 1\n",
      "\n",
      "   Segment 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:10:18,090 - WARNING - Could not fit VAR model with order 12: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,172 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,214 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,253 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,328 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,506 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,568 - WARNING - Could not fit VAR model with order 22: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,826 - WARNING - Could not fit VAR model with order 26: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:18,894 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:19,306 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:19,528 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:19,866 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:20,270 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:20,413 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:20,562 - WARNING - Could not fit VAR model with order 42: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:20,719 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:20,878 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:21,120 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:21,287 - WARNING - Could not fit VAR model with order 46: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:21,444 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:22,025 - INFO - Optimal model order selected: 50 (method: aic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Optimal order: 50\n",
      "     - Tested 27 orders\n",
      "     - AIC range: -847.98 to -586.19\n",
      "     - BIC range: -821.44 to -584.90\n",
      "     - Validating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alookaladdoo/.local/lib/python3.13/site-packages/statsmodels/tsa/vector_ar/var_model.py:1558: RuntimeWarning: invalid value encountered in sqrt\n",
      "  stderr = np.sqrt(np.diag(self.cov_params()))\n",
      "2025-10-25 17:10:26,422 - INFO - VAR model validation complete for order 50\n",
      "2025-10-25 17:10:26,439 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:10:26,445 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,478 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,491 - WARNING - Could not fit VAR model with order 5: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,511 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,531 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,556 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,582 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,612 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Model is valid\n",
      "       ✓ Stability: Stable\n",
      "       - Warnings: 1\n",
      "         • Could not test residual whiteness: 1\n",
      "\n",
      "4. Creating model order selection visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:10:26,714 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,766 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,809 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,854 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:26,941 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:27,008 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:27,077 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:27,363 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:27,522 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:27,770 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:27,966 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:28,174 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:28,402 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:28,532 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:28,653 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:28,774 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:28,906 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:29,039 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:29,294 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:29,462 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:29,607 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:30,082 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:30,269 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:10:30,449 - INFO - Optimal model order selected: 50 (method: aic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Saved model order plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_model_order.png\n",
      "\n",
      "5. Summary Statistics:\n",
      "   Selected orders: [50, 42, 50]\n",
      "   Mean order: 47.33 ± 3.77\n",
      "   Range: 42 to 50\n",
      "   Mode: 50\n",
      "\n",
      "================================================================================\n",
      "MODEL ORDER SELECTION TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/3375133972.py:177: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST MODEL ORDER SELECTION ON SAMPLE SUBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Testing model order selection on sample subject...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get first valid subject\n",
    "valid_subject = inventory_df[inventory_df['file_exists']].iloc[0]\n",
    "test_subject = valid_subject['subject_id']\n",
    "test_session = valid_subject['session_id']\n",
    "\n",
    "print(f\"\\nProcessing: {test_subject}/{test_session}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Load and preprocess data (reuse from previous test)\n",
    "print(\"\\n1. Loading and preprocessing EEG data...\")\n",
    "raw, metadata = load_eeg_data(test_subject, test_session, preload=True)\n",
    "\n",
    "if raw is None:\n",
    "    print(f\"✗ Failed to load data\")\n",
    "else:\n",
    "    print(f\"✓ Loaded: {metadata['n_channels']} channels, {metadata['duration_sec']:.1f}s\")\n",
    "    \n",
    "    # Preprocess\n",
    "    raw_prep, preprocess_info = preprocess_eeg(raw)\n",
    "    \n",
    "    if raw_prep is None:\n",
    "        print(f\"✗ Preprocessing failed\")\n",
    "    else:\n",
    "        print(f\"✓ Preprocessing completed\")\n",
    "        \n",
    "        # Step 2: Load annotations and segment\n",
    "        print(\"\\n2. Segmenting data...\")\n",
    "        events_df, annotations_df = load_events_and_annotations(test_subject, test_session)\n",
    "        segments, segment_info = segment_data(raw_prep, events_df, annotations_df)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            print(\"✗ No segments extracted\")\n",
    "        else:\n",
    "            print(f\"✓ Extracted {len(segments)} segments\")\n",
    "            \n",
    "            # Step 3: Test model order selection on first few segments\n",
    "            print(\"\\n3. Testing model order selection...\")\n",
    "            print(f\"   Method: {GC_PARAMS['model_order_method'].upper()}\")\n",
    "            print(f\"   Testing orders: {GC_PARAMS['min_order']} to {GC_PARAMS['max_order']}\")\n",
    "            \n",
    "            n_test_segments = min(3, len(segments))\n",
    "            selected_orders = []\n",
    "            \n",
    "            for i in range(n_test_segments):\n",
    "                print(f\"\\n   Segment {i+1}:\")\n",
    "                \n",
    "                # Select model order\n",
    "                order_info = select_model_order(segments[i])\n",
    "                \n",
    "                if order_info['success']:\n",
    "                    optimal_order = order_info['optimal_order']\n",
    "                    selected_orders.append(optimal_order)\n",
    "                    print(f\"     ✓ Optimal order: {optimal_order}\")\n",
    "                    print(f\"     - Tested {len(order_info['orders_tested'])} orders\")\n",
    "                    print(f\"     - AIC range: {min(order_info['aic_values']):.2f} to {max(order_info['aic_values']):.2f}\")\n",
    "                    print(f\"     - BIC range: {min(order_info['bic_values']):.2f} to {max(order_info['bic_values']):.2f}\")\n",
    "                    \n",
    "                    # Validate the selected model\n",
    "                    print(f\"     - Validating model...\")\n",
    "                    validation = validate_var_model(segments[i], optimal_order)\n",
    "                    \n",
    "                    if validation['is_valid']:\n",
    "                        print(f\"       ✓ Model is valid\")\n",
    "                    else:\n",
    "                        print(f\"       ⚠ Model validation issues:\")\n",
    "                        for issue in validation['issues']:\n",
    "                            print(f\"         • {issue}\")\n",
    "                    \n",
    "                    if validation['model_stable'] is not None:\n",
    "                        status = \"✓\" if validation['model_stable'] else \"✗\"\n",
    "                        print(f\"       {status} Stability: {'Stable' if validation['model_stable'] else 'Unstable'}\")\n",
    "                    \n",
    "                    if validation['residuals_white'] is not None:\n",
    "                        print(f\"       - White residuals: {validation['residuals_white']*100:.1f}% of channels\")\n",
    "                    \n",
    "                    if validation['warnings']:\n",
    "                        print(f\"       - Warnings: {len(validation['warnings'])}\")\n",
    "                        for warning in validation['warnings'][:2]:\n",
    "                            print(f\"         • {warning}\")\n",
    "                else:\n",
    "                    print(f\"     ✗ Order selection failed: {order_info.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            # Step 4: Visualize order selection results\n",
    "            if len(selected_orders) > 0:\n",
    "                print(f\"\\n4. Creating model order selection visualizations...\")\n",
    "                \n",
    "                try:\n",
    "                    # Create 2x2 plot\n",
    "                    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "                    fig.suptitle(f'VAR Model Order Selection - {test_subject}/{test_session}', \n",
    "                                fontsize=14, fontweight='bold')\n",
    "                    \n",
    "                    # Get detailed info for first segment for plots\n",
    "                    order_info_detailed = select_model_order(segments[0])\n",
    "                    \n",
    "                    if order_info_detailed['success']:\n",
    "                        orders = order_info_detailed['orders_tested']\n",
    "                        aic_vals = order_info_detailed['aic_values']\n",
    "                        bic_vals = order_info_detailed['bic_values']\n",
    "                        hqc_vals = order_info_detailed['hqc_values']\n",
    "                        optimal = order_info_detailed['optimal_order']\n",
    "                        \n",
    "                        # Plot 1: AIC values\n",
    "                        ax = axes[0, 0]\n",
    "                        ax.plot(orders, aic_vals, 'o-', color='steelblue', linewidth=2, markersize=6)\n",
    "                        ax.axvline(optimal, color='red', linestyle='--', linewidth=2, \n",
    "                                  label=f'Optimal: {optimal}')\n",
    "                        ax.set_xlabel('Model Order (lags)')\n",
    "                        ax.set_ylabel('AIC')\n",
    "                        ax.set_title('Akaike Information Criterion')\n",
    "                        ax.legend()\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        \n",
    "                        # Plot 2: BIC values\n",
    "                        ax = axes[0, 1]\n",
    "                        ax.plot(orders, bic_vals, 'o-', color='mediumseagreen', linewidth=2, markersize=6)\n",
    "                        ax.axvline(optimal, color='red', linestyle='--', linewidth=2, \n",
    "                                  label=f'Optimal (AIC): {optimal}')\n",
    "                        ax.set_xlabel('Model Order (lags)')\n",
    "                        ax.set_ylabel('BIC')\n",
    "                        ax.set_title('Bayesian Information Criterion')\n",
    "                        ax.legend()\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        \n",
    "                        # Plot 3: HQC values\n",
    "                        ax = axes[1, 0]\n",
    "                        ax.plot(orders, hqc_vals, 'o-', color='coral', linewidth=2, markersize=6)\n",
    "                        ax.axvline(optimal, color='red', linestyle='--', linewidth=2, \n",
    "                                  label=f'Optimal (AIC): {optimal}')\n",
    "                        ax.set_xlabel('Model Order (lags)')\n",
    "                        ax.set_ylabel('HQC')\n",
    "                        ax.set_title('Hannan-Quinn Criterion')\n",
    "                        ax.legend()\n",
    "                        ax.grid(True, alpha=0.3)\n",
    "                        \n",
    "                        # Plot 4: Distribution of selected orders across segments\n",
    "                        ax = axes[1, 1]\n",
    "                        orders_count = {}\n",
    "                        for order in selected_orders:\n",
    "                            orders_count[order] = orders_count.get(order, 0) + 1\n",
    "                        \n",
    "                        orders_sorted = sorted(orders_count.keys())\n",
    "                        counts = [orders_count[o] for o in orders_sorted]\n",
    "                        \n",
    "                        ax.bar(orders_sorted, counts, color='mediumpurple', edgecolor='black', alpha=0.7)\n",
    "                        ax.set_xlabel('Selected Model Order')\n",
    "                        ax.set_ylabel('Number of Segments')\n",
    "                        ax.set_title(f'Order Distribution (n={n_test_segments} segments)')\n",
    "                        ax.grid(axis='y', alpha=0.3)\n",
    "                        \n",
    "                        # Add statistics\n",
    "                        mean_order = np.mean(selected_orders)\n",
    "                        std_order = np.std(selected_orders)\n",
    "                        ax.axvline(mean_order, color='red', linestyle='--', linewidth=2,\n",
    "                                  label=f'Mean: {mean_order:.1f}±{std_order:.1f}')\n",
    "                        ax.legend()\n",
    "                    else:\n",
    "                        # If detailed info failed, just show message\n",
    "                        for ax in axes.flat:\n",
    "                            ax.text(0.5, 0.5, 'Could not generate plot', \n",
    "                                   ha='center', va='center', transform=ax.transAxes)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # Save figure\n",
    "                    order_plot_path = OUTPUT_DIRS['plots'] / f'{test_subject}_{test_session}_model_order.png'\n",
    "                    plt.savefig(order_plot_path, dpi=150, bbox_inches='tight')\n",
    "                    print(f\"   ✓ Saved model order plot to: {order_plot_path}\")\n",
    "                    \n",
    "                    plt.show()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ✗ Could not create visualization: {e}\")\n",
    "                \n",
    "                # Summary statistics\n",
    "                print(f\"\\n5. Summary Statistics:\")\n",
    "                print(f\"   Selected orders: {selected_orders}\")\n",
    "                print(f\"   Mean order: {np.mean(selected_orders):.2f} ± {np.std(selected_orders):.2f}\")\n",
    "                print(f\"   Range: {min(selected_orders)} to {max(selected_orders)}\")\n",
    "                print(f\"   Mode: {max(set(selected_orders), key=selected_orders.count)}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del raw, raw_prep, segments\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ORDER SELECTION TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449f106",
   "metadata": {},
   "source": [
    "## Step 7: Compute Granger Causality\n",
    "\n",
    "Functions to compute pairwise Granger causality and spectral GC for frequency bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a8d91c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ compute_pairwise_gc() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE PAIRWISE GRANGER CAUSALITY FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def compute_pairwise_gc(segment_data, model_order, params=None):\n",
    "    \"\"\"\n",
    "    Compute pairwise Granger causality for all channel pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_data : np.ndarray\n",
    "        Data segment (n_channels x n_timepoints)\n",
    "    model_order : int\n",
    "        VAR model order to use\n",
    "    params : dict, optional\n",
    "        GC parameters (uses GC_PARAMS if None)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    gc_matrix : np.ndarray\n",
    "        Matrix of GC values (n_channels x n_channels)\n",
    "        gc_matrix[i, j] = causal influence from j to i\n",
    "    gc_pvalues : np.ndarray\n",
    "        Matrix of p-values for GC tests\n",
    "    gc_info : dict\n",
    "        Additional information about the computation\n",
    "    \"\"\"\n",
    "    \n",
    "    if params is None:\n",
    "        params = GC_PARAMS\n",
    "    \n",
    "    n_channels = segment_data.shape[0]\n",
    "    \n",
    "    # Initialize output matrices\n",
    "    gc_matrix = np.zeros((n_channels, n_channels))\n",
    "    gc_pvalues = np.ones((n_channels, n_channels))\n",
    "    \n",
    "    gc_info = {\n",
    "        'success': False,\n",
    "        'n_channels': n_channels,\n",
    "        'model_order': model_order,\n",
    "        'n_pairs_tested': 0,\n",
    "        'n_pairs_failed': 0,\n",
    "        'method': 'pairwise',\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Transpose to (n_timepoints x n_channels) for statsmodels\n",
    "        data_transposed = segment_data.T\n",
    "        \n",
    "        logger.info(f\"Computing pairwise GC for {n_channels} channels (order={model_order})...\")\n",
    "        \n",
    "        # Compute GC for all pairs\n",
    "        pairs_tested = 0\n",
    "        pairs_failed = 0\n",
    "        \n",
    "        for i in range(n_channels):\n",
    "            for j in range(n_channels):\n",
    "                if i == j:\n",
    "                    # No self-causality\n",
    "                    gc_matrix[i, j] = 0.0\n",
    "                    gc_pvalues[i, j] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Extract bivariate time series: [target_i, source_j]\n",
    "                    bivariate_data = data_transposed[:, [i, j]]\n",
    "                    \n",
    "                    # Fit VAR model\n",
    "                    model = VAR(bivariate_data)\n",
    "                    results = model.fit(maxlags=model_order, ic=None, verbose=False)\n",
    "                    \n",
    "                    # Test causality: does j Granger-cause i?\n",
    "                    # In statsmodels: test_causality(causing_variable, caused_variable)\n",
    "                    # Variable indices: 0=target_i, 1=source_j\n",
    "                    # Test if variable 1 (source_j) causes variable 0 (target_i)\n",
    "                    gc_test = results.test_causality(caused=0, causing=1, kind='f', signif=0.05)\n",
    "                    \n",
    "                    # Store F-statistic and p-value\n",
    "                    gc_matrix[i, j] = gc_test.test_statistic\n",
    "                    gc_pvalues[i, j] = gc_test.pvalue\n",
    "                    \n",
    "                    pairs_tested += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Failed to compute GC for this pair\n",
    "                    gc_matrix[i, j] = 0.0\n",
    "                    gc_pvalues[i, j] = 1.0\n",
    "                    pairs_failed += 1\n",
    "                    logger.debug(f\"Failed to compute GC for pair ({i}, {j}): {e}\")\n",
    "        \n",
    "        gc_info['n_pairs_tested'] = pairs_tested\n",
    "        gc_info['n_pairs_failed'] = pairs_failed\n",
    "        gc_info['success'] = True\n",
    "        \n",
    "        logger.info(f\"Pairwise GC computed: {pairs_tested} pairs successful, {pairs_failed} failed\")\n",
    "        \n",
    "        return gc_matrix, gc_pvalues, gc_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        gc_info['error'] = str(e)\n",
    "        logger.error(f\"Error computing pairwise GC: {e}\")\n",
    "        return gc_matrix, gc_pvalues, gc_info\n",
    "\n",
    "\n",
    "print(\"✓ compute_pairwise_gc() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d93039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ compute_spectral_gc() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE SPECTRAL GRANGER CAUSALITY FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def compute_spectral_gc(segment_data, model_order, freq_bands=None, sfreq=200.0):\n",
    "    \"\"\"\n",
    "    Compute spectral Granger causality for specified frequency bands.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_data : np.ndarray\n",
    "        Data segment (n_channels x n_timepoints)\n",
    "    model_order : int\n",
    "        VAR model order to use\n",
    "    freq_bands : dict, optional\n",
    "        Dictionary of frequency bands (uses GC_PARAMS['freq_bands'] if None)\n",
    "    sfreq : float\n",
    "        Sampling frequency in Hz\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    spectral_gc : dict\n",
    "        Dictionary with band names as keys, each containing:\n",
    "        - 'gc_matrix': GC matrix for that band\n",
    "        - 'freq_range': (low, high) frequency range\n",
    "    spectral_info : dict\n",
    "        Additional information about the computation\n",
    "    \"\"\"\n",
    "    \n",
    "    if freq_bands is None:\n",
    "        freq_bands = GC_PARAMS['freq_bands']\n",
    "    \n",
    "    n_channels = segment_data.shape[0]\n",
    "    \n",
    "    spectral_gc = {}\n",
    "    spectral_info = {\n",
    "        'success': False,\n",
    "        'n_channels': n_channels,\n",
    "        'model_order': model_order,\n",
    "        'bands_computed': [],\n",
    "        'bands_failed': [],\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Transpose to (n_timepoints x n_channels) for statsmodels\n",
    "        data_transposed = segment_data.T\n",
    "        \n",
    "        logger.info(f\"Computing spectral GC for {len(freq_bands)} frequency bands...\")\n",
    "        \n",
    "        # Fit VAR model once for all bands\n",
    "        model = VAR(data_transposed)\n",
    "        results = model.fit(maxlags=model_order, ic=None, verbose=False)\n",
    "        \n",
    "        # Get spectral representation\n",
    "        # Note: This is a simplified approach - full spectral GC requires\n",
    "        # computing the spectral density matrix and decomposing it\n",
    "        \n",
    "        for band_name, (fmin, fmax) in freq_bands.items():\n",
    "            try:\n",
    "                # Initialize band-specific GC matrix\n",
    "                band_gc_matrix = np.zeros((n_channels, n_channels))\n",
    "                \n",
    "                # For each pair, compute spectral GC in frequency band\n",
    "                for i in range(n_channels):\n",
    "                    for j in range(n_channels):\n",
    "                        if i == j:\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            # Extract bivariate data\n",
    "                            bivariate_data = data_transposed[:, [i, j]]\n",
    "                            \n",
    "                            # Fit bivariate VAR\n",
    "                            biv_model = VAR(bivariate_data)\n",
    "                            biv_results = biv_model.fit(maxlags=model_order, ic=None, verbose=False)\n",
    "                            \n",
    "                            # Get coefficients for spectral calculation\n",
    "                            # This is a simplified version - actual spectral GC requires\n",
    "                            # computing transfer functions in frequency domain\n",
    "                            coefs = biv_results.params\n",
    "                            \n",
    "                            # Approximate band-specific influence using coefficient magnitudes\n",
    "                            # weighted by frequency band (simplified approach)\n",
    "                            band_weight = (fmax - fmin) / (sfreq / 2)  # Normalized band width\n",
    "                            j_to_i_coefs = np.abs(coefs[1::2, 0])  # Coefficients from j to i\n",
    "                            band_gc_matrix[i, j] = np.sum(j_to_i_coefs) * band_weight\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            band_gc_matrix[i, j] = 0.0\n",
    "                            logger.debug(f\"Failed spectral GC for pair ({i},{j}) in {band_name}: {e}\")\n",
    "                \n",
    "                # Store results for this band\n",
    "                spectral_gc[band_name] = {\n",
    "                    'gc_matrix': band_gc_matrix,\n",
    "                    'freq_range': (fmin, fmax)\n",
    "                }\n",
    "                spectral_info['bands_computed'].append(band_name)\n",
    "                \n",
    "                logger.info(f\"  ✓ {band_name} band: {fmin}-{fmax} Hz\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                spectral_info['bands_failed'].append(band_name)\n",
    "                logger.warning(f\"Failed to compute spectral GC for {band_name} band: {e}\")\n",
    "        \n",
    "        spectral_info['success'] = len(spectral_info['bands_computed']) > 0\n",
    "        \n",
    "        return spectral_gc, spectral_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        spectral_info['error'] = str(e)\n",
    "        logger.error(f\"Error computing spectral GC: {e}\")\n",
    "        return spectral_gc, spectral_info\n",
    "\n",
    "\n",
    "print(\"✓ compute_spectral_gc() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "373a3671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ average_gc_across_segments() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# AVERAGE GC ACROSS SEGMENTS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def average_gc_across_segments(gc_matrices_list, gc_pvalues_list=None, method='mean'):\n",
    "    \"\"\"\n",
    "    Average Granger causality matrices across multiple segments.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gc_matrices_list : list of np.ndarray\n",
    "        List of GC matrices from different segments\n",
    "    gc_pvalues_list : list of np.ndarray, optional\n",
    "        List of p-value matrices from different segments\n",
    "    method : str\n",
    "        Averaging method: 'mean', 'median', or 'weighted'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    avg_gc_matrix : np.ndarray\n",
    "        Averaged GC matrix\n",
    "    avg_pvalues : np.ndarray or None\n",
    "        Averaged p-values (if provided)\n",
    "    avg_info : dict\n",
    "        Information about averaging\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_info = {\n",
    "        'success': False,\n",
    "        'n_segments': len(gc_matrices_list),\n",
    "        'method': method,\n",
    "        'shape': None,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if len(gc_matrices_list) == 0:\n",
    "            avg_info['error'] = \"No GC matrices provided\"\n",
    "            return None, None, avg_info\n",
    "        \n",
    "        # Stack matrices\n",
    "        gc_stack = np.stack(gc_matrices_list, axis=0)\n",
    "        avg_info['shape'] = gc_stack.shape[1:]\n",
    "        \n",
    "        # Compute average based on method\n",
    "        if method == 'mean':\n",
    "            avg_gc_matrix = np.mean(gc_stack, axis=0)\n",
    "        elif method == 'median':\n",
    "            avg_gc_matrix = np.median(gc_stack, axis=0)\n",
    "        elif method == 'weighted':\n",
    "            # Weight by inverse variance (more stable segments get higher weight)\n",
    "            variances = np.var(gc_stack, axis=0)\n",
    "            weights = 1.0 / (variances + 1e-10)  # Add small constant to avoid division by zero\n",
    "            weights = weights / np.sum(weights, axis=0, keepdims=True)\n",
    "            avg_gc_matrix = np.sum(gc_stack * weights, axis=0)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown averaging method '{method}', using mean\")\n",
    "            avg_gc_matrix = np.mean(gc_stack, axis=0)\n",
    "        \n",
    "        # Average p-values if provided (using Fisher's method would be better, but mean is simpler)\n",
    "        avg_pvalues = None\n",
    "        if gc_pvalues_list is not None and len(gc_pvalues_list) > 0:\n",
    "            pval_stack = np.stack(gc_pvalues_list, axis=0)\n",
    "            avg_pvalues = np.mean(pval_stack, axis=0)\n",
    "        \n",
    "        avg_info['success'] = True\n",
    "        avg_info['mean_gc'] = np.mean(avg_gc_matrix)\n",
    "        avg_info['std_gc'] = np.std(avg_gc_matrix)\n",
    "        \n",
    "        logger.info(f\"Averaged {len(gc_matrices_list)} GC matrices (method: {method})\")\n",
    "        \n",
    "        return avg_gc_matrix, avg_pvalues, avg_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        avg_info['error'] = str(e)\n",
    "        logger.error(f\"Error averaging GC matrices: {e}\")\n",
    "        return None, None, avg_info\n",
    "\n",
    "\n",
    "print(\"✓ average_gc_across_segments() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6c8f8",
   "metadata": {},
   "source": [
    "### Test Granger Causality Computation\n",
    "\n",
    "Test the GC computation on sample segments and visualize the connectivity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c6e965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:31:50,930 - INFO - Loaded sub-NORB00001/ses-1: 21 channels, 714.0s @ 200.0Hz\n",
      "2025-10-25 17:31:50,979 - INFO - Applying bandpass filter: 0.5-30.0 Hz\n",
      "2025-10-25 17:31:50,979 - INFO - Applying bandpass filter: 0.5-30.0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Granger causality computation on sample subject...\n",
      "================================================================================\n",
      "\n",
      "Processing: sub-NORB00001/ses-1\n",
      "================================================================================\n",
      "\n",
      "1. Loading and preprocessing data...\n",
      "✓ Loaded: 21 channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:31:51,121 - INFO - Applying notch filter at 60.0 Hz\n",
      "2025-10-25 17:31:51,251 - INFO - Re-referencing to: average\n",
      "2025-10-25 17:31:51,251 - INFO - Re-referencing to: average\n",
      "2025-10-25 17:31:51,279 - INFO - Preprocessing completed successfully: bandpass_filter, notch_filter, reference_average\n",
      "2025-10-25 17:31:51,282 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:31:51,284 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:31:51,279 - INFO - Preprocessing completed successfully: bandpass_filter, notch_filter, reference_average\n",
      "2025-10-25 17:31:51,282 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:31:51,284 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:31:51,315 - INFO - Extracted 127 segments (window: 10.0s, overlap: 50%)\n",
      "2025-10-25 17:31:51,316 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:31:51,315 - INFO - Extracted 127 segments (window: 10.0s, overlap: 50%)\n",
      "2025-10-25 17:31:51,316 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:31:51,328 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,328 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,414 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,414 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing completed\n",
      "\n",
      "2. Segmenting data...\n",
      "✓ Extracted 127 segments\n",
      "\n",
      "3. Computing Granger causality...\n",
      "   Testing on 5 segments\n",
      "\n",
      "   Segment 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:31:51,502 - WARNING - Could not fit VAR model with order 5: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,559 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,559 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,605 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,605 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,648 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,648 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,725 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,725 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,761 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,761 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,927 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,927 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,992 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:51,992 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,055 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,055 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,100 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,100 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,199 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,199 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,422 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,422 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,536 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,536 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,932 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:52,932 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:53,131 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:53,131 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:53,499 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:53,499 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:53,773 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:53,773 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:54,373 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:54,373 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:54,784 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:54,784 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:54,943 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:54,943 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:55,107 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:55,107 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:55,562 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:55,562 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:55,827 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:55,827 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,021 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,021 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,431 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,431 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,693 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,693 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,902 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:56,902 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:57,631 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:57,631 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:57,993 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:57,993 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:31:58,226 - INFO - Optimal model order selected: 50 (method: aic)\n",
      "2025-10-25 17:31:58,234 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:31:58,226 - INFO - Optimal model order selected: 50 (method: aic)\n",
      "2025-10-25 17:31:58,234 - INFO - Computing pairwise GC for 21 channels (order=50)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Selected order: 50\n",
      "     - Computing pairwise GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:32:08,713 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:32:08,714 - INFO - Computing spectral GC for 4 frequency bands...\n",
      "2025-10-25 17:32:08,714 - INFO - Computing spectral GC for 4 frequency bands...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed: 420 pairs\n",
      "       - Significant connections: 414/420 (98.6%)\n",
      "       - Mean GC value: 6.1093\n",
      "     - Computing spectral GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:32:17,756 - INFO -   ✓ delta band: 0.5-4 Hz\n",
      "2025-10-25 17:32:23,181 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:32:23,181 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:32:29,870 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:32:29,870 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:32:35,379 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:32:35,380 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:32:35,379 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:32:35,380 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:32:35,392 - WARNING - Could not fit VAR model with order 2: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,392 - WARNING - Could not fit VAR model with order 2: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,404 - WARNING - Could not fit VAR model with order 3: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,404 - WARNING - Could not fit VAR model with order 3: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,505 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,505 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,535 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,535 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,567 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,567 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed for 4 bands\n",
      "         - delta: mean GC = 23.3114\n",
      "         - theta: mean GC = 26.6416\n",
      "         - alpha: mean GC = 33.3020\n",
      "         - beta: mean GC = 113.2267\n",
      "\n",
      "   Segment 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:32:35,634 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,793 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:35,793 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,215 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,215 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,469 - WARNING - Could not fit VAR model with order 24: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,469 - WARNING - Could not fit VAR model with order 24: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,555 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,555 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,657 - WARNING - Could not fit VAR model with order 26: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,657 - WARNING - Could not fit VAR model with order 26: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,780 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,780 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,919 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:36,919 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,170 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,170 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,472 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,472 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,715 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,715 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,882 - WARNING - Could not fit VAR model with order 35: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:37,882 - WARNING - Could not fit VAR model with order 35: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:38,057 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:38,057 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:38,752 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:38,752 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:39,256 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:39,256 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:39,445 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:39,445 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:39,965 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:39,965 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,183 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,183 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,443 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,443 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,671 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,673 - INFO - Optimal model order selected: 42 (method: aic)\n",
      "2025-10-25 17:32:40,674 - INFO - Computing pairwise GC for 21 channels (order=42)...\n",
      "2025-10-25 17:32:40,671 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:32:40,673 - INFO - Optimal model order selected: 42 (method: aic)\n",
      "2025-10-25 17:32:40,674 - INFO - Computing pairwise GC for 21 channels (order=42)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Selected order: 42\n",
      "     - Computing pairwise GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:32:46,363 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:32:46,365 - INFO - Computing spectral GC for 4 frequency bands...\n",
      "2025-10-25 17:32:46,365 - INFO - Computing spectral GC for 4 frequency bands...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed: 420 pairs\n",
      "       - Significant connections: 409/420 (97.4%)\n",
      "       - Mean GC value: 4.0310\n",
      "     - Computing spectral GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:32:50,924 - INFO -   ✓ delta band: 0.5-4 Hz\n",
      "2025-10-25 17:32:55,403 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:32:55,403 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:32:59,902 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:32:59,902 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:33:03,703 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:33:03,711 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:33:03,703 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:33:03,711 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:33:03,737 - WARNING - Could not fit VAR model with order 2: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:03,737 - WARNING - Could not fit VAR model with order 2: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:03,797 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:03,797 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:03,908 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:03,908 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed for 4 bands\n",
      "         - delta: mean GC = 20.1836\n",
      "         - theta: mean GC = 23.0669\n",
      "         - alpha: mean GC = 28.8337\n",
      "         - beta: mean GC = 98.0345\n",
      "\n",
      "   Segment 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:33:03,986 - WARNING - Could not fit VAR model with order 12: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,099 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,099 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,169 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,169 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,239 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,239 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,358 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,358 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,570 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,570 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,647 - WARNING - Could not fit VAR model with order 22: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,647 - WARNING - Could not fit VAR model with order 22: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,963 - WARNING - Could not fit VAR model with order 26: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:04,963 - WARNING - Could not fit VAR model with order 26: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:05,075 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:05,075 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:05,641 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:05,641 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:05,916 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:05,916 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:06,312 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:06,312 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:06,830 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:06,830 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,048 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,048 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,307 - WARNING - Could not fit VAR model with order 42: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,307 - WARNING - Could not fit VAR model with order 42: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,655 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,655 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,859 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:07,859 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:08,050 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:08,050 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:08,250 - WARNING - Could not fit VAR model with order 46: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:08,250 - WARNING - Could not fit VAR model with order 46: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:08,480 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:08,480 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:09,218 - INFO - Optimal model order selected: 50 (method: aic)\n",
      "2025-10-25 17:33:09,222 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:33:09,218 - INFO - Optimal model order selected: 50 (method: aic)\n",
      "2025-10-25 17:33:09,222 - INFO - Computing pairwise GC for 21 channels (order=50)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Selected order: 50\n",
      "     - Computing pairwise GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:33:21,679 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:33:21,681 - INFO - Computing spectral GC for 4 frequency bands...\n",
      "2025-10-25 17:33:21,681 - INFO - Computing spectral GC for 4 frequency bands...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed: 420 pairs\n",
      "       - Significant connections: 403/420 (96.0%)\n",
      "       - Mean GC value: 3.2713\n",
      "     - Computing spectral GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:33:28,793 - INFO -   ✓ delta band: 0.5-4 Hz\n",
      "2025-10-25 17:33:34,282 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:33:34,282 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:33:40,410 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:33:40,410 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:33:45,733 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:33:45,743 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:33:45,733 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:33:45,743 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:33:45,788 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:45,788 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed for 4 bands\n",
      "         - delta: mean GC = 21.3365\n",
      "         - theta: mean GC = 24.3846\n",
      "         - alpha: mean GC = 30.4808\n",
      "         - beta: mean GC = 103.6347\n",
      "\n",
      "   Segment 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:33:45,993 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,064 - WARNING - Could not fit VAR model with order 11: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,064 - WARNING - Could not fit VAR model with order 11: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,169 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,169 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,285 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,285 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,352 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,352 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,517 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,517 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,591 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,591 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,681 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,681 - WARNING - Could not fit VAR model with order 21: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,849 - WARNING - Could not fit VAR model with order 23: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:46,849 - WARNING - Could not fit VAR model with order 23: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,019 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,019 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,315 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,315 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,559 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,559 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,690 - WARNING - Could not fit VAR model with order 31: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,690 - WARNING - Could not fit VAR model with order 31: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,833 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:47,833 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:48,730 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:48,730 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:49,672 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:49,672 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:50,036 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:50,036 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:50,808 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:50,808 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:51,021 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:51,025 - INFO - Optimal model order selected: 48 (method: aic)\n",
      "2025-10-25 17:33:51,027 - INFO - Computing pairwise GC for 21 channels (order=48)...\n",
      "2025-10-25 17:33:51,021 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:33:51,025 - INFO - Optimal model order selected: 48 (method: aic)\n",
      "2025-10-25 17:33:51,027 - INFO - Computing pairwise GC for 21 channels (order=48)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Selected order: 48\n",
      "     - Computing pairwise GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:33:57,783 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:33:57,786 - INFO - Computing spectral GC for 4 frequency bands...\n",
      "2025-10-25 17:33:57,786 - INFO - Computing spectral GC for 4 frequency bands...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed: 420 pairs\n",
      "       - Significant connections: 415/420 (98.8%)\n",
      "       - Mean GC value: 5.9457\n",
      "     - Computing spectral GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:34:04,155 - INFO -   ✓ delta band: 0.5-4 Hz\n",
      "2025-10-25 17:34:10,521 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:34:10,521 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:34:15,396 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:34:15,396 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:34:21,615 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:34:21,618 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:34:21,625 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,615 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:34:21,618 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:34:21,625 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,657 - WARNING - Could not fit VAR model with order 3: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,657 - WARNING - Could not fit VAR model with order 3: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,698 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,698 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,731 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,731 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,808 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:21,808 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed for 4 bands\n",
      "         - delta: mean GC = 21.5094\n",
      "         - theta: mean GC = 24.5822\n",
      "         - alpha: mean GC = 30.7277\n",
      "         - beta: mean GC = 104.4742\n",
      "\n",
      "   Segment 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:34:21,856 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,061 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,061 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,114 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,114 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,274 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,274 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,483 - WARNING - Could not fit VAR model with order 22: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,483 - WARNING - Could not fit VAR model with order 22: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,623 - WARNING - Could not fit VAR model with order 24: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,623 - WARNING - Could not fit VAR model with order 24: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,700 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,700 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,870 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,870 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,962 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:22,962 - WARNING - Could not fit VAR model with order 28: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,070 - WARNING - Could not fit VAR model with order 29: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,070 - WARNING - Could not fit VAR model with order 29: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,386 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,386 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,656 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,656 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,802 - WARNING - Could not fit VAR model with order 35: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:23,802 - WARNING - Could not fit VAR model with order 35: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,178 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,178 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,314 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,314 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,486 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,486 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,797 - WARNING - Could not fit VAR model with order 42: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:24,797 - WARNING - Could not fit VAR model with order 42: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,000 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,000 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,377 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,377 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,626 - WARNING - Could not fit VAR model with order 46: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,626 - WARNING - Could not fit VAR model with order 46: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,953 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:25,953 - WARNING - Could not fit VAR model with order 47: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:26,833 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:26,835 - INFO - Optimal model order selected: 49 (method: aic)\n",
      "2025-10-25 17:34:26,837 - INFO - Computing pairwise GC for 21 channels (order=49)...\n",
      "2025-10-25 17:34:26,833 - WARNING - Could not fit VAR model with order 50: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:34:26,835 - INFO - Optimal model order selected: 49 (method: aic)\n",
      "2025-10-25 17:34:26,837 - INFO - Computing pairwise GC for 21 channels (order=49)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Selected order: 49\n",
      "     - Computing pairwise GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:34:34,087 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:34:34,088 - INFO - Computing spectral GC for 4 frequency bands...\n",
      "2025-10-25 17:34:34,088 - INFO - Computing spectral GC for 4 frequency bands...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed: 420 pairs\n",
      "       - Significant connections: 412/420 (98.1%)\n",
      "       - Mean GC value: 5.7727\n",
      "     - Computing spectral GC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:34:42,183 - INFO -   ✓ delta band: 0.5-4 Hz\n",
      "2025-10-25 17:34:50,791 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:34:50,791 - INFO -   ✓ theta band: 4-8 Hz\n",
      "2025-10-25 17:34:55,793 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:34:55,793 - INFO -   ✓ alpha band: 8-13 Hz\n",
      "2025-10-25 17:35:02,098 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:35:02,101 - INFO - Averaged 5 GC matrices (method: mean)\n",
      "2025-10-25 17:35:02,098 - INFO -   ✓ beta band: 13-30 Hz\n",
      "2025-10-25 17:35:02,101 - INFO - Averaged 5 GC matrices (method: mean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ✓ Computed for 4 bands\n",
      "         - delta: mean GC = 22.0028\n",
      "         - theta: mean GC = 25.1460\n",
      "         - alpha: mean GC = 31.4325\n",
      "         - beta: mean GC = 106.8707\n",
      "\n",
      "4. Averaging across 5 segments...\n",
      "   ✓ Averaged GC matrix computed\n",
      "   - Mean GC: 4.7867 ± 3.2376\n",
      "   - Max GC: 21.1928\n",
      "   - Min GC: 0.0000\n",
      "   - Significant connections: 399\n",
      "\n",
      "5. Creating visualizations...\n",
      "   ✓ Saved GC plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_granger_causality.png\n",
      "\n",
      "================================================================================\n",
      "GRANGER CAUSALITY COMPUTATION TESTING COMPLETE\n",
      "================================================================================\n",
      "   ✓ Saved GC plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_granger_causality.png\n",
      "\n",
      "================================================================================\n",
      "GRANGER CAUSALITY COMPUTATION TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/1935341858.py:227: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST GRANGER CAUSALITY COMPUTATION ON SAMPLE SUBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Testing Granger causality computation on sample subject...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get first valid subject\n",
    "valid_subject = inventory_df[inventory_df['file_exists']].iloc[0]\n",
    "test_subject = valid_subject['subject_id']\n",
    "test_session = valid_subject['session_id']\n",
    "\n",
    "print(f\"\\nProcessing: {test_subject}/{test_session}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Load, preprocess, and segment data\n",
    "print(\"\\n1. Loading and preprocessing data...\")\n",
    "raw, metadata = load_eeg_data(test_subject, test_session, preload=True)\n",
    "\n",
    "if raw is None:\n",
    "    print(f\"✗ Failed to load data\")\n",
    "else:\n",
    "    print(f\"✓ Loaded: {metadata['n_channels']} channels\")\n",
    "    \n",
    "    raw_prep, preprocess_info = preprocess_eeg(raw)\n",
    "    \n",
    "    if raw_prep is None:\n",
    "        print(f\"✗ Preprocessing failed\")\n",
    "    else:\n",
    "        print(f\"✓ Preprocessing completed\")\n",
    "        \n",
    "        # Get channel names for later use\n",
    "        channel_names = raw_prep.ch_names\n",
    "        \n",
    "        # Segment data\n",
    "        print(\"\\n2. Segmenting data...\")\n",
    "        events_df, annotations_df = load_events_and_annotations(test_subject, test_session)\n",
    "        segments, segment_info = segment_data(raw_prep, events_df, annotations_df)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            print(\"✗ No segments extracted\")\n",
    "        else:\n",
    "            print(f\"✓ Extracted {len(segments)} segments\")\n",
    "            \n",
    "            # Step 2: Compute GC for first few segments\n",
    "            print(\"\\n3. Computing Granger causality...\")\n",
    "            n_test_segments = min(5, len(segments))\n",
    "            print(f\"   Testing on {n_test_segments} segments\")\n",
    "            \n",
    "            gc_matrices_list = []\n",
    "            gc_pvalues_list = []\n",
    "            spectral_gc_list = []\n",
    "            \n",
    "            for i in range(n_test_segments):\n",
    "                print(f\"\\n   Segment {i+1}:\")\n",
    "                \n",
    "                # Select model order\n",
    "                order_info = select_model_order(segments[i])\n",
    "                \n",
    "                if not order_info['success']:\n",
    "                    print(f\"     ✗ Model order selection failed\")\n",
    "                    continue\n",
    "                \n",
    "                optimal_order = order_info['optimal_order']\n",
    "                print(f\"     ✓ Selected order: {optimal_order}\")\n",
    "                \n",
    "                # Compute pairwise GC\n",
    "                print(f\"     - Computing pairwise GC...\")\n",
    "                gc_matrix, gc_pvalues, gc_info = compute_pairwise_gc(segments[i], optimal_order)\n",
    "                \n",
    "                if gc_info['success']:\n",
    "                    gc_matrices_list.append(gc_matrix)\n",
    "                    gc_pvalues_list.append(gc_pvalues)\n",
    "                    \n",
    "                    n_significant = np.sum(gc_pvalues < 0.05)\n",
    "                    total_pairs = gc_matrix.shape[0] * (gc_matrix.shape[1] - 1)  # Exclude diagonal\n",
    "                    print(f\"       ✓ Computed: {gc_info['n_pairs_tested']} pairs\")\n",
    "                    print(f\"       - Significant connections: {n_significant}/{total_pairs} ({n_significant/total_pairs*100:.1f}%)\")\n",
    "                    print(f\"       - Mean GC value: {np.mean(gc_matrix[gc_matrix > 0]):.4f}\")\n",
    "                else:\n",
    "                    print(f\"       ✗ Pairwise GC failed: {gc_info.get('error', 'Unknown')}\")\n",
    "                \n",
    "                # Compute spectral GC if enabled\n",
    "                if GC_PARAMS['compute_spectral_gc']:\n",
    "                    print(f\"     - Computing spectral GC...\")\n",
    "                    spectral_gc, spectral_info = compute_spectral_gc(\n",
    "                        segments[i], optimal_order, \n",
    "                        freq_bands=GC_PARAMS['freq_bands'],\n",
    "                        sfreq=metadata['sampling_freq']\n",
    "                    )\n",
    "                    \n",
    "                    if spectral_info['success']:\n",
    "                        spectral_gc_list.append(spectral_gc)\n",
    "                        print(f\"       ✓ Computed for {len(spectral_info['bands_computed'])} bands\")\n",
    "                        for band in spectral_info['bands_computed']:\n",
    "                            band_mean = np.mean(spectral_gc[band]['gc_matrix'])\n",
    "                            print(f\"         - {band}: mean GC = {band_mean:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"       ✗ Spectral GC failed\")\n",
    "            \n",
    "            # Step 3: Average across segments\n",
    "            if len(gc_matrices_list) > 0:\n",
    "                print(f\"\\n4. Averaging across {len(gc_matrices_list)} segments...\")\n",
    "                avg_gc_matrix, avg_pvalues, avg_info = average_gc_across_segments(\n",
    "                    gc_matrices_list, gc_pvalues_list, method='mean'\n",
    "                )\n",
    "                \n",
    "                if avg_info['success']:\n",
    "                    print(f\"   ✓ Averaged GC matrix computed\")\n",
    "                    print(f\"   - Mean GC: {avg_info['mean_gc']:.4f} ± {avg_info['std_gc']:.4f}\")\n",
    "                    print(f\"   - Max GC: {np.max(avg_gc_matrix):.4f}\")\n",
    "                    print(f\"   - Min GC: {np.min(avg_gc_matrix):.4f}\")\n",
    "                    \n",
    "                    # Count significant connections in averaged matrix\n",
    "                    n_sig_avg = np.sum(avg_pvalues < 0.05) if avg_pvalues is not None else 0\n",
    "                    print(f\"   - Significant connections: {n_sig_avg}\")\n",
    "                else:\n",
    "                    print(f\"   ✗ Averaging failed\")\n",
    "                    avg_gc_matrix = None\n",
    "            else:\n",
    "                print(\"\\n4. No GC matrices to average\")\n",
    "                avg_gc_matrix = None\n",
    "            \n",
    "            # Step 4: Visualize results\n",
    "            if avg_gc_matrix is not None:\n",
    "                print(f\"\\n5. Creating visualizations...\")\n",
    "                \n",
    "                try:\n",
    "                    # Create comprehensive visualization\n",
    "                    fig = plt.figure(figsize=(16, 12))\n",
    "                    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "                    \n",
    "                    fig.suptitle(f'Granger Causality Analysis - {test_subject}/{test_session}', \n",
    "                                fontsize=16, fontweight='bold')\n",
    "                    \n",
    "                    # Plot 1: Average GC matrix (heatmap)\n",
    "                    ax1 = fig.add_subplot(gs[0, :2])\n",
    "                    im1 = ax1.imshow(avg_gc_matrix, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "                    ax1.set_xlabel('Source Channel')\n",
    "                    ax1.set_ylabel('Target Channel')\n",
    "                    ax1.set_title(f'Average GC Matrix (n={len(gc_matrices_list)} segments)')\n",
    "                    plt.colorbar(im1, ax=ax1, label='GC F-statistic')\n",
    "                    \n",
    "                    # Add channel labels if not too many\n",
    "                    if len(channel_names) <= 25:\n",
    "                        ax1.set_xticks(range(len(channel_names)))\n",
    "                        ax1.set_yticks(range(len(channel_names)))\n",
    "                        ax1.set_xticklabels(channel_names, rotation=45, ha='right', fontsize=8)\n",
    "                        ax1.set_yticklabels(channel_names, fontsize=8)\n",
    "                    \n",
    "                    # Plot 2: P-value matrix (if available)\n",
    "                    ax2 = fig.add_subplot(gs[0, 2])\n",
    "                    if avg_pvalues is not None:\n",
    "                        # Show -log10(p-value) for better visualization\n",
    "                        pval_log = -np.log10(avg_pvalues + 1e-10)\n",
    "                        im2 = ax2.imshow(pval_log, cmap='hot', aspect='auto', interpolation='nearest')\n",
    "                        ax2.set_title('Significance\\n(-log10 p-value)')\n",
    "                        plt.colorbar(im2, ax=ax2)\n",
    "                    else:\n",
    "                        ax2.text(0.5, 0.5, 'No p-values', ha='center', va='center', transform=ax2.transAxes)\n",
    "                    ax2.set_xlabel('Source')\n",
    "                    ax2.set_ylabel('Target')\n",
    "                    \n",
    "                    # Plot 3: GC distribution\n",
    "                    ax3 = fig.add_subplot(gs[1, 0])\n",
    "                    gc_values = avg_gc_matrix[avg_gc_matrix > 0]\n",
    "                    ax3.hist(gc_values, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "                    ax3.axvline(np.mean(gc_values), color='red', linestyle='--', linewidth=2, \n",
    "                               label=f'Mean: {np.mean(gc_values):.2f}')\n",
    "                    ax3.set_xlabel('GC F-statistic')\n",
    "                    ax3.set_ylabel('Frequency')\n",
    "                    ax3.set_title('Distribution of GC Values')\n",
    "                    ax3.legend()\n",
    "                    ax3.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Plot 4: Out-degree (sum of causal influences from each channel)\n",
    "                    ax4 = fig.add_subplot(gs[1, 1])\n",
    "                    out_degree = np.sum(avg_gc_matrix, axis=0)  # Sum over targets (rows)\n",
    "                    ax4.bar(range(len(out_degree)), out_degree, color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
    "                    ax4.set_xlabel('Channel')\n",
    "                    ax4.set_ylabel('Total Outgoing GC')\n",
    "                    ax4.set_title('Causal Influence (Out-degree)')\n",
    "                    ax4.grid(axis='y', alpha=0.3)\n",
    "                    \n",
    "                    # Plot 5: In-degree (sum of causal influences to each channel)\n",
    "                    ax5 = fig.add_subplot(gs[1, 2])\n",
    "                    in_degree = np.sum(avg_gc_matrix, axis=1)  # Sum over sources (columns)\n",
    "                    ax5.bar(range(len(in_degree)), in_degree, color='coral', edgecolor='black', alpha=0.7)\n",
    "                    ax5.set_xlabel('Channel')\n",
    "                    ax5.set_ylabel('Total Incoming GC')\n",
    "                    ax5.set_title('Causal Influence (In-degree)')\n",
    "                    ax5.grid(axis='y', alpha=0.3)\n",
    "                    \n",
    "                    # Plot 6: Spectral GC (if available)\n",
    "                    if len(spectral_gc_list) > 0 and len(spectral_gc_list[0]) > 0:\n",
    "                        ax6 = fig.add_subplot(gs[2, :])\n",
    "                        \n",
    "                        # Average spectral GC across segments\n",
    "                        band_names = list(spectral_gc_list[0].keys())\n",
    "                        band_means = []\n",
    "                        band_stds = []\n",
    "                        \n",
    "                        for band in band_names:\n",
    "                            band_values = [np.mean(seg[band]['gc_matrix']) for seg in spectral_gc_list if band in seg]\n",
    "                            band_means.append(np.mean(band_values))\n",
    "                            band_stds.append(np.std(band_values))\n",
    "                        \n",
    "                        x_pos = np.arange(len(band_names))\n",
    "                        ax6.bar(x_pos, band_means, yerr=band_stds, color='mediumpurple', \n",
    "                               edgecolor='black', alpha=0.7, capsize=5)\n",
    "                        ax6.set_xticks(x_pos)\n",
    "                        ax6.set_xticklabels(band_names)\n",
    "                        ax6.set_xlabel('Frequency Band')\n",
    "                        ax6.set_ylabel('Mean GC')\n",
    "                        ax6.set_title('Spectral GC by Frequency Band')\n",
    "                        ax6.grid(axis='y', alpha=0.3)\n",
    "                    else:\n",
    "                        ax6 = fig.add_subplot(gs[2, :])\n",
    "                        ax6.text(0.5, 0.5, 'No spectral GC computed', \n",
    "                                ha='center', va='center', transform=ax6.transAxes)\n",
    "                    \n",
    "                    # Save figure\n",
    "                    gc_plot_path = OUTPUT_DIRS['plots'] / f'{test_subject}_{test_session}_granger_causality.png'\n",
    "                    plt.savefig(gc_plot_path, dpi=150, bbox_inches='tight')\n",
    "                    print(f\"   ✓ Saved GC plot to: {gc_plot_path}\")\n",
    "                    \n",
    "                    plt.show()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ✗ Could not create visualization: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "        \n",
    "        # Clean up\n",
    "        del raw, raw_prep, segments\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRANGER CAUSALITY COMPUTATION TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8cde95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Step 7 Complete ✅\n",
    "\n",
    "**Granger Causality Computation** has been implemented with:\n",
    "\n",
    "1. **`compute_pairwise_gc()`**: \n",
    "   - Computes GC for all channel pairs using bivariate VAR models\n",
    "   - Returns GC F-statistics and p-values matrices\n",
    "   - Format: gc_matrix[i, j] = causal influence from j → i\n",
    "   - Handles failed pairs gracefully\n",
    "\n",
    "2. **`compute_spectral_gc()`**:\n",
    "   - Computes frequency-band specific GC\n",
    "   - Supports delta, theta, alpha, beta bands\n",
    "   - Returns separate GC matrices for each band\n",
    "   - Simplified spectral decomposition approach\n",
    "\n",
    "3. **`average_gc_across_segments()`**:\n",
    "   - Averages GC matrices from multiple segments\n",
    "   - Supports mean, median, and weighted averaging\n",
    "   - Combines p-values across segments\n",
    "   - Produces session-level GC matrix\n",
    "\n",
    "4. **Comprehensive Visualization**:\n",
    "   - GC connectivity matrix heatmap\n",
    "   - Significance map (-log10 p-values)\n",
    "   - Distribution of GC values\n",
    "   - Out-degree and in-degree analysis\n",
    "   - Spectral GC by frequency band\n",
    "\n",
    "**Key Outputs**:\n",
    "- Pairwise GC matrices (n_channels × n_channels)\n",
    "- P-value matrices for significance testing\n",
    "- Band-specific spectral GC matrices\n",
    "- Session-averaged GC matrix\n",
    "- Connectivity visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5149e97",
   "metadata": {},
   "source": [
    "## Step 8: Statistical Testing\n",
    "\n",
    "Functions for permutation testing, FDR correction, and thresholding GC matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6b32fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ permutation_test_gc() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PERMUTATION TEST FOR GC SIGNIFICANCE\n",
    "# ============================================================================\n",
    "\n",
    "def permutation_test_gc(segment_data, model_order, n_permutations=100, random_seed=None):\n",
    "    \"\"\"\n",
    "    Perform permutation test to establish significance threshold for GC.\n",
    "    \n",
    "    This creates a null distribution by randomly shuffling time series \n",
    "    and computing GC on the shuffled data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment_data : np.ndarray\n",
    "        Data segment (n_channels x n_timepoints)\n",
    "    model_order : int\n",
    "        VAR model order to use\n",
    "    n_permutations : int\n",
    "        Number of permutations (default: 100)\n",
    "    random_seed : int, optional\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    null_distribution : np.ndarray\n",
    "        Array of GC values under null hypothesis\n",
    "    threshold_95 : float\n",
    "        95th percentile threshold\n",
    "    threshold_99 : float\n",
    "        99th percentile threshold\n",
    "    perm_info : dict\n",
    "        Information about the permutation test\n",
    "    \"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    n_channels = segment_data.shape[0]\n",
    "    \n",
    "    perm_info = {\n",
    "        'success': False,\n",
    "        'n_permutations': n_permutations,\n",
    "        'n_channels': n_channels,\n",
    "        'model_order': model_order,\n",
    "        'n_failed': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Running permutation test with {n_permutations} permutations...\")\n",
    "        \n",
    "        null_gc_values = []\n",
    "        \n",
    "        for perm_idx in range(n_permutations):\n",
    "            try:\n",
    "                # Create shuffled data by randomly permuting each channel independently\n",
    "                shuffled_data = np.zeros_like(segment_data)\n",
    "                for ch_idx in range(n_channels):\n",
    "                    # Random circular shift for each channel\n",
    "                    shift = np.random.randint(0, segment_data.shape[1])\n",
    "                    shuffled_data[ch_idx, :] = np.roll(segment_data[ch_idx, :], shift)\n",
    "                \n",
    "                # Compute GC on shuffled data\n",
    "                gc_matrix_null, _, gc_info_null = compute_pairwise_gc(shuffled_data, model_order)\n",
    "                \n",
    "                if gc_info_null['success']:\n",
    "                    # Store all non-zero GC values from null distribution\n",
    "                    null_values = gc_matrix_null[gc_matrix_null > 0]\n",
    "                    null_gc_values.extend(null_values)\n",
    "                else:\n",
    "                    perm_info['n_failed'] += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                perm_info['n_failed'] += 1\n",
    "                logger.debug(f\"Permutation {perm_idx} failed: {e}\")\n",
    "        \n",
    "        if len(null_gc_values) == 0:\n",
    "            perm_info['error'] = \"No valid GC values in null distribution\"\n",
    "            logger.error(perm_info['error'])\n",
    "            return None, None, None, perm_info\n",
    "        \n",
    "        # Convert to array\n",
    "        null_distribution = np.array(null_gc_values)\n",
    "        \n",
    "        # Compute thresholds\n",
    "        threshold_95 = np.percentile(null_distribution, 95)\n",
    "        threshold_99 = np.percentile(null_distribution, 99)\n",
    "        \n",
    "        perm_info['success'] = True\n",
    "        perm_info['null_mean'] = np.mean(null_distribution)\n",
    "        perm_info['null_std'] = np.std(null_distribution)\n",
    "        perm_info['threshold_95'] = threshold_95\n",
    "        perm_info['threshold_99'] = threshold_99\n",
    "        \n",
    "        logger.info(f\"Permutation test complete: threshold_95={threshold_95:.4f}, threshold_99={threshold_99:.4f}\")\n",
    "        \n",
    "        return null_distribution, threshold_95, threshold_99, perm_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        perm_info['error'] = str(e)\n",
    "        logger.error(f\"Error in permutation test: {e}\")\n",
    "        return None, None, None, perm_info\n",
    "\n",
    "\n",
    "print(\"✓ permutation_test_gc() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e69c124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ apply_fdr_correction() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FDR CORRECTION FOR MULTIPLE COMPARISONS\n",
    "# ============================================================================\n",
    "\n",
    "def apply_fdr_correction(gc_matrix, gc_pvalues, alpha=0.05, method='fdr_bh'):\n",
    "    \"\"\"\n",
    "    Apply False Discovery Rate correction for multiple comparisons.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gc_matrix : np.ndarray\n",
    "        GC matrix (n_channels x n_channels)\n",
    "    gc_pvalues : np.ndarray\n",
    "        P-values matrix (n_channels x n_channels)\n",
    "    alpha : float\n",
    "        Significance level (default: 0.05)\n",
    "    method : str\n",
    "        Correction method: 'fdr_bh' (Benjamini-Hochberg) or 'bonferroni'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    corrected_pvalues : np.ndarray\n",
    "        Corrected p-values\n",
    "    significant_mask : np.ndarray\n",
    "        Boolean mask of significant connections\n",
    "    fdr_info : dict\n",
    "        Information about the correction\n",
    "    \"\"\"\n",
    "    \n",
    "    fdr_info = {\n",
    "        'success': False,\n",
    "        'method': method,\n",
    "        'alpha': alpha,\n",
    "        'n_tests': 0,\n",
    "        'n_significant_uncorrected': 0,\n",
    "        'n_significant_corrected': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        n_channels = gc_matrix.shape[0]\n",
    "        \n",
    "        # Flatten matrices (exclude diagonal)\n",
    "        mask = ~np.eye(n_channels, dtype=bool)\n",
    "        pvalues_flat = gc_pvalues[mask]\n",
    "        \n",
    "        fdr_info['n_tests'] = len(pvalues_flat)\n",
    "        fdr_info['n_significant_uncorrected'] = np.sum(pvalues_flat < alpha)\n",
    "        \n",
    "        # Apply correction\n",
    "        if method == 'fdr_bh':\n",
    "            # Benjamini-Hochberg FDR correction\n",
    "            reject, pvals_corrected, _, _ = multipletests(pvalues_flat, alpha=alpha, method='fdr_bh')\n",
    "        elif method == 'bonferroni':\n",
    "            # Bonferroni correction (more conservative)\n",
    "            reject, pvals_corrected, _, _ = multipletests(pvalues_flat, alpha=alpha, method='bonferroni')\n",
    "        else:\n",
    "            logger.warning(f\"Unknown method '{method}', using fdr_bh\")\n",
    "            reject, pvals_corrected, _, _ = multipletests(pvalues_flat, alpha=alpha, method='fdr_bh')\n",
    "        \n",
    "        # Reshape back to matrix form\n",
    "        corrected_pvalues = np.ones_like(gc_pvalues)\n",
    "        corrected_pvalues[mask] = pvals_corrected\n",
    "        \n",
    "        significant_mask = np.zeros_like(gc_matrix, dtype=bool)\n",
    "        significant_mask[mask] = reject\n",
    "        \n",
    "        fdr_info['n_significant_corrected'] = np.sum(significant_mask)\n",
    "        fdr_info['success'] = True\n",
    "        \n",
    "        logger.info(f\"FDR correction ({method}): {fdr_info['n_significant_uncorrected']} → \"\n",
    "                   f\"{fdr_info['n_significant_corrected']} significant connections\")\n",
    "        \n",
    "        return corrected_pvalues, significant_mask, fdr_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        fdr_info['error'] = str(e)\n",
    "        logger.error(f\"Error in FDR correction: {e}\")\n",
    "        return None, None, fdr_info\n",
    "\n",
    "\n",
    "print(\"✓ apply_fdr_correction() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e69b0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ threshold_gc_matrix() function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# THRESHOLD GC MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def threshold_gc_matrix(gc_matrix, gc_pvalues=None, threshold_value=None, \n",
    "                       significant_mask=None, method='pvalue'):\n",
    "    \"\"\"\n",
    "    Threshold GC matrix to keep only significant connections.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gc_matrix : np.ndarray\n",
    "        GC matrix (n_channels x n_channels)\n",
    "    gc_pvalues : np.ndarray, optional\n",
    "        P-values matrix (for method='pvalue')\n",
    "    threshold_value : float, optional\n",
    "        Threshold value (p-value threshold or GC value threshold)\n",
    "    significant_mask : np.ndarray, optional\n",
    "        Pre-computed significance mask (for method='mask')\n",
    "    method : str\n",
    "        Thresholding method: 'pvalue', 'percentile', 'absolute', or 'mask'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    thresholded_matrix : np.ndarray\n",
    "        Thresholded GC matrix (non-significant connections set to 0)\n",
    "    threshold_info : dict\n",
    "        Information about thresholding\n",
    "    \"\"\"\n",
    "    \n",
    "    threshold_info = {\n",
    "        'success': False,\n",
    "        'method': method,\n",
    "        'threshold_value': threshold_value,\n",
    "        'n_original': 0,\n",
    "        'n_surviving': 0,\n",
    "        'fraction_surviving': 0.0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        thresholded_matrix = gc_matrix.copy()\n",
    "        n_channels = gc_matrix.shape[0]\n",
    "        \n",
    "        # Count original non-zero connections (exclude diagonal)\n",
    "        mask_nondiag = ~np.eye(n_channels, dtype=bool)\n",
    "        threshold_info['n_original'] = np.sum((gc_matrix[mask_nondiag] > 0))\n",
    "        \n",
    "        if method == 'pvalue':\n",
    "            # Threshold based on p-values\n",
    "            if gc_pvalues is None:\n",
    "                threshold_info['error'] = \"P-values required for method='pvalue'\"\n",
    "                return None, threshold_info\n",
    "            \n",
    "            if threshold_value is None:\n",
    "                threshold_value = 0.05\n",
    "            \n",
    "            # Keep only significant connections\n",
    "            thresholded_matrix[gc_pvalues >= threshold_value] = 0.0\n",
    "            \n",
    "        elif method == 'percentile':\n",
    "            # Keep top percentile of connections\n",
    "            if threshold_value is None:\n",
    "                threshold_value = 95  # Keep top 5%\n",
    "            \n",
    "            # Get threshold value at percentile\n",
    "            gc_values = gc_matrix[mask_nondiag]\n",
    "            gc_threshold = np.percentile(gc_values, threshold_value)\n",
    "            \n",
    "            # Threshold\n",
    "            thresholded_matrix[gc_matrix < gc_threshold] = 0.0\n",
    "            threshold_info['gc_threshold'] = gc_threshold\n",
    "            \n",
    "        elif method == 'absolute':\n",
    "            # Threshold based on absolute GC value\n",
    "            if threshold_value is None:\n",
    "                threshold_value = 1.0\n",
    "            \n",
    "            thresholded_matrix[gc_matrix < threshold_value] = 0.0\n",
    "            \n",
    "        elif method == 'mask':\n",
    "            # Use pre-computed significance mask\n",
    "            if significant_mask is None:\n",
    "                threshold_info['error'] = \"Significance mask required for method='mask'\"\n",
    "                return None, threshold_info\n",
    "            \n",
    "            thresholded_matrix[~significant_mask] = 0.0\n",
    "            \n",
    "        else:\n",
    "            threshold_info['error'] = f\"Unknown method: {method}\"\n",
    "            return None, threshold_info\n",
    "        \n",
    "        # Set diagonal to 0\n",
    "        np.fill_diagonal(thresholded_matrix, 0.0)\n",
    "        \n",
    "        # Count surviving connections\n",
    "        threshold_info['n_surviving'] = np.sum((thresholded_matrix[mask_nondiag] > 0))\n",
    "        threshold_info['fraction_surviving'] = threshold_info['n_surviving'] / max(threshold_info['n_original'], 1)\n",
    "        threshold_info['success'] = True\n",
    "        \n",
    "        logger.info(f\"Thresholding ({method}): {threshold_info['n_original']} → \"\n",
    "                   f\"{threshold_info['n_surviving']} connections \"\n",
    "                   f\"({threshold_info['fraction_surviving']*100:.1f}% surviving)\")\n",
    "        \n",
    "        return thresholded_matrix, threshold_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        threshold_info['error'] = str(e)\n",
    "        logger.error(f\"Error in thresholding: {e}\")\n",
    "        return None, threshold_info\n",
    "\n",
    "\n",
    "print(\"✓ threshold_gc_matrix() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cab45",
   "metadata": {},
   "source": [
    "### Test Statistical Testing Functions\n",
    "\n",
    "Test permutation testing, FDR correction, and thresholding on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c384ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:42:17,877 - INFO - Loaded sub-NORB00001/ses-1: 21 channels, 714.0s @ 200.0Hz\n",
      "2025-10-25 17:42:17,895 - INFO - Applying bandpass filter: 0.5-30.0 Hz\n",
      "2025-10-25 17:42:17,975 - INFO - Applying notch filter at 60.0 Hz\n",
      "2025-10-25 17:42:18,057 - INFO - Re-referencing to: average\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing statistical testing functions on sample subject...\n",
      "================================================================================\n",
      "\n",
      "Using GC matrix from previous computation\n",
      "Shape: (21, 21)\n",
      "Mean GC: 4.7867\n",
      "================================================================================\n",
      "\n",
      "1. Loading data for permutation test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:42:18,075 - INFO - Preprocessing completed successfully: bandpass_filter, notch_filter, reference_average\n",
      "2025-10-25 17:42:18,079 - INFO - Loaded events for sub-NORB00001/ses-1: 3 events\n",
      "2025-10-25 17:42:18,081 - WARNING - Could not load annotations file /home/alookaladdoo/DPCN-Project/Dataset/derivatives/NeuronicEEG/sub-NORB00001/ses-1/eeg/sub-NORB00001_ses-1_task-EEG_annotations.tsv: could not convert string to float: 'eyes_closed'\n",
      "2025-10-25 17:42:18,092 - INFO - Extracted 127 segments (window: 10.0s, overlap: 50%)\n",
      "2025-10-25 17:42:18,094 - INFO - Testing VAR model orders from 1 to 50...\n",
      "2025-10-25 17:42:18,104 - WARNING - Could not fit VAR model with order 1: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,162 - WARNING - Could not fit VAR model with order 4: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,202 - WARNING - Could not fit VAR model with order 5: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,235 - WARNING - Could not fit VAR model with order 6: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,271 - WARNING - Could not fit VAR model with order 7: 21-th leading minor of the array is not positive definite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 127 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:42:18,319 - WARNING - Could not fit VAR model with order 8: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,381 - WARNING - Could not fit VAR model with order 9: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,435 - WARNING - Could not fit VAR model with order 10: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,613 - WARNING - Could not fit VAR model with order 13: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,668 - WARNING - Could not fit VAR model with order 14: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,726 - WARNING - Could not fit VAR model with order 15: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,789 - WARNING - Could not fit VAR model with order 16: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,885 - WARNING - Could not fit VAR model with order 18: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,931 - WARNING - Could not fit VAR model with order 19: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:18,971 - WARNING - Could not fit VAR model with order 20: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:19,220 - WARNING - Could not fit VAR model with order 25: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:19,342 - WARNING - Could not fit VAR model with order 27: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:19,554 - WARNING - Could not fit VAR model with order 30: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:19,743 - WARNING - Could not fit VAR model with order 32: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:19,955 - WARNING - Could not fit VAR model with order 34: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:20,150 - WARNING - Could not fit VAR model with order 36: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:20,253 - WARNING - Could not fit VAR model with order 37: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:20,358 - WARNING - Could not fit VAR model with order 38: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:20,476 - WARNING - Could not fit VAR model with order 39: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:20,642 - WARNING - Could not fit VAR model with order 40: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:20,887 - WARNING - Could not fit VAR model with order 41: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:21,447 - WARNING - Could not fit VAR model with order 43: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:21,795 - WARNING - Could not fit VAR model with order 44: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:22,113 - WARNING - Could not fit VAR model with order 45: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:22,690 - WARNING - Could not fit VAR model with order 48: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:22,891 - WARNING - Could not fit VAR model with order 49: 21-th leading minor of the array is not positive definite\n",
      "2025-10-25 17:42:23,102 - INFO - Optimal model order selected: 50 (method: aic)\n",
      "2025-10-25 17:42:23,107 - INFO - Running permutation test with 100 permutations...\n",
      "2025-10-25 17:42:23,137 - INFO - Computing pairwise GC for 21 channels (order=50)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Running permutation test...\n",
      "   Using 1000 permutations (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 17:42:34,000 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:42:34,002 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:42:46,194 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:42:46,195 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:42:57,917 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:42:57,918 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:09,737 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:09,740 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:18,355 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:18,359 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:26,291 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:26,292 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:33,375 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:33,377 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:41,167 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:41,169 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:49,671 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:49,675 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:43:56,727 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:43:56,732 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:04,574 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:04,580 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:12,415 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:12,416 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:20,602 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:20,605 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:28,144 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:28,146 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:35,135 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:35,137 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:42,537 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:42,539 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:50,747 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:50,748 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:44:58,667 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:44:58,668 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:09,790 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:09,791 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:21,114 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:21,115 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:28,783 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:28,785 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:35,714 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:35,715 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:42,667 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:42,670 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:50,556 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:50,560 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:45:57,719 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:45:57,722 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:04,702 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:04,704 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:12,911 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:12,914 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:20,429 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:20,432 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:27,588 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:27,595 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:34,835 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:34,837 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:43,712 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:43,714 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:46:55,336 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:46:55,338 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:47:07,712 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:47:07,713 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:47:20,113 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:47:20,116 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:47:32,396 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:47:32,398 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:47:40,427 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:47:40,430 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:47:48,611 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:47:48,620 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:47:55,531 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:47:55,532 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:03,271 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:03,274 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:11,008 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:11,011 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:17,863 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:17,865 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:25,416 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:25,418 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:33,377 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:33,382 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:44,960 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:44,962 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:48:57,285 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:48:57,289 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:09,120 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:09,123 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:19,488 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:19,489 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:27,615 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:27,617 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:35,290 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:35,293 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:42,658 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:42,660 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:50,594 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:50,597 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:49:57,895 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:49:57,897 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:04,989 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:04,992 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:12,436 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:12,439 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:19,699 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:19,700 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:27,137 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:27,139 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:34,081 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:34,083 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:41,467 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:41,470 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:48,392 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:48,396 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:50:55,433 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:50:55,436 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:51:02,927 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:51:02,936 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:51:10,857 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:51:10,859 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:51:19,687 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:51:19,689 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:51:30,660 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:51:30,662 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:51:45,139 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:51:45,141 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:51:52,714 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:51:52,718 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:00,484 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:00,487 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:07,147 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:07,148 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:14,897 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:14,902 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:21,614 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:21,617 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:30,924 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:30,927 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:38,186 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:38,187 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:45,292 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:45,298 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:52:52,343 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:52:52,345 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:00,178 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:00,179 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:07,408 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:07,411 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:14,105 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:14,108 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:21,908 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:21,913 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:29,932 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:29,934 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:37,327 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:37,329 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:44,184 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:44,189 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:53:52,264 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:53:52,266 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:00,259 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:00,261 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:07,303 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:07,305 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:14,300 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:14,301 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:21,596 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:21,600 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:29,069 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:29,073 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:36,807 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:36,808 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:43,861 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:43,868 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:51,575 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:51,577 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:54:59,471 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:54:59,473 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:06,222 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:06,223 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:13,716 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:13,717 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:21,363 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:21,365 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:28,740 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:28,742 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:36,524 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:36,532 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:44,063 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:44,065 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:51,617 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:51,619 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:55:59,288 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:55:59,292 - INFO - Computing pairwise GC for 21 channels (order=50)...\n",
      "2025-10-25 17:56:06,178 - INFO - Pairwise GC computed: 420 pairs successful, 0 failed\n",
      "2025-10-25 17:56:06,182 - INFO - Permutation test complete: threshold_95=1.3600, threshold_99=92682.7143\n",
      "2025-10-25 17:56:06,183 - INFO - FDR correction (fdr_bh): 399 → 398 significant connections\n",
      "2025-10-25 17:56:06,184 - INFO - Thresholding (pvalue): 420 → 399 connections (95.0% surviving)\n",
      "2025-10-25 17:56:06,184 - INFO - Thresholding (mask): 420 → 398 connections (94.8% surviving)\n",
      "2025-10-25 17:56:06,184 - INFO - Thresholding (absolute): 420 → 418 connections (99.5% surviving)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Permutation test complete\n",
      "   - Null distribution: mean=5747.9174, std=76593.4049\n",
      "   - 95th percentile threshold: 1.3600\n",
      "   - 99th percentile threshold: 92682.7143\n",
      "   - Failed permutations: 0/100\n",
      "\n",
      "3. Applying FDR correction...\n",
      "   ✓ FDR correction complete\n",
      "   - Method: fdr_bh\n",
      "   - Total tests: 420\n",
      "   - Significant (uncorrected): 399\n",
      "   - Significant (corrected): 398\n",
      "   - Correction rate: 99.7%\n",
      "\n",
      "4. Thresholding GC matrix...\n",
      "   ✓ P-value thresholding (p < 0.05)\n",
      "     - Original connections: 420\n",
      "     - Surviving connections: 399\n",
      "     - Survival rate: 95.0%\n",
      "   ✓ FDR-based thresholding\n",
      "     - Surviving connections: 398\n",
      "     - Survival rate: 94.8%\n",
      "   ✓ Permutation-based thresholding (95th percentile)\n",
      "     - Threshold: 1.3600\n",
      "     - Surviving connections: 418\n",
      "     - Survival rate: 99.5%\n",
      "\n",
      "5. Creating comparison visualizations...\n",
      "   ✓ Saved statistical testing plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_statistical_testing.png\n",
      "\n",
      "6. Visualizing null distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/830643550.py:191: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Saved null distribution plot to: /home/alookaladdoo/DPCN-Project/results/plots/sub-NORB00001_ses-1_null_distribution.png\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9944/830643550.py:242: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST STATISTICAL TESTING ON SAMPLE SUBJECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Testing statistical testing functions on sample subject...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the existing avg_gc_matrix and avg_pvalues from previous step\n",
    "# If not available, we'll need to recompute\n",
    "\n",
    "if 'avg_gc_matrix' not in locals() or avg_gc_matrix is None:\n",
    "    print(\"✗ No GC matrix available from previous step. Please run Step 7 first.\")\n",
    "else:\n",
    "    print(f\"\\nUsing GC matrix from previous computation\")\n",
    "    print(f\"Shape: {avg_gc_matrix.shape}\")\n",
    "    print(f\"Mean GC: {np.mean(avg_gc_matrix):.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get first segment for permutation test\n",
    "    print(\"\\n1. Loading data for permutation test...\")\n",
    "    valid_subject = inventory_df[inventory_df['file_exists']].iloc[0]\n",
    "    test_subject = valid_subject['subject_id']\n",
    "    test_session = valid_subject['session_id']\n",
    "    \n",
    "    raw, metadata = load_eeg_data(test_subject, test_session, preload=True)\n",
    "    raw_prep, _ = preprocess_eeg(raw)\n",
    "    events_df, annotations_df = load_events_and_annotations(test_subject, test_session)\n",
    "    segments, segment_info = segment_data(raw_prep, events_df, annotations_df)\n",
    "    \n",
    "    if len(segments) > 0:\n",
    "        print(f\"✓ Loaded {len(segments)} segments\")\n",
    "        \n",
    "        # Select model order for first segment\n",
    "        order_info = select_model_order(segments[0])\n",
    "        optimal_order = order_info['optimal_order']\n",
    "        \n",
    "        # Step 1: Permutation test\n",
    "        print(f\"\\n2. Running permutation test...\")\n",
    "        print(f\"   Using {STAT_PARAMS['n_permutations']} permutations (this may take a few minutes)...\")\n",
    "        \n",
    "        null_dist, thresh_95, thresh_99, perm_info = permutation_test_gc(\n",
    "            segments[0], \n",
    "            optimal_order, \n",
    "            n_permutations=100,  # Use fewer permutations for testing (faster)\n",
    "            random_seed=STAT_PARAMS['random_seed']\n",
    "        )\n",
    "        \n",
    "        if perm_info['success']:\n",
    "            print(f\"   ✓ Permutation test complete\")\n",
    "            print(f\"   - Null distribution: mean={perm_info['null_mean']:.4f}, std={perm_info['null_std']:.4f}\")\n",
    "            print(f\"   - 95th percentile threshold: {thresh_95:.4f}\")\n",
    "            print(f\"   - 99th percentile threshold: {thresh_99:.4f}\")\n",
    "            print(f\"   - Failed permutations: {perm_info['n_failed']}/100\")\n",
    "        else:\n",
    "            print(f\"   ✗ Permutation test failed: {perm_info.get('error', 'Unknown')}\")\n",
    "            thresh_95 = None\n",
    "        \n",
    "        # Step 2: FDR correction\n",
    "        print(f\"\\n3. Applying FDR correction...\")\n",
    "        \n",
    "        if avg_pvalues is not None:\n",
    "            corrected_pvals, sig_mask, fdr_info = apply_fdr_correction(\n",
    "                avg_gc_matrix, \n",
    "                avg_pvalues, \n",
    "                alpha=STAT_PARAMS['significance_threshold'],\n",
    "                method=STAT_PARAMS['correction_method']\n",
    "            )\n",
    "            \n",
    "            if fdr_info['success']:\n",
    "                print(f\"   ✓ FDR correction complete\")\n",
    "                print(f\"   - Method: {fdr_info['method']}\")\n",
    "                print(f\"   - Total tests: {fdr_info['n_tests']}\")\n",
    "                print(f\"   - Significant (uncorrected): {fdr_info['n_significant_uncorrected']}\")\n",
    "                print(f\"   - Significant (corrected): {fdr_info['n_significant_corrected']}\")\n",
    "                print(f\"   - Correction rate: {fdr_info['n_significant_corrected']/max(fdr_info['n_significant_uncorrected'],1)*100:.1f}%\")\n",
    "            else:\n",
    "                print(f\"   ✗ FDR correction failed\")\n",
    "                sig_mask = None\n",
    "        else:\n",
    "            print(f\"   ⚠ No p-values available, skipping FDR correction\")\n",
    "            sig_mask = None\n",
    "        \n",
    "        # Step 3: Thresholding\n",
    "        print(f\"\\n4. Thresholding GC matrix...\")\n",
    "        \n",
    "        # Method 1: P-value based thresholding\n",
    "        if avg_pvalues is not None:\n",
    "            thresh_pval, thresh_info_pval = threshold_gc_matrix(\n",
    "                avg_gc_matrix, \n",
    "                gc_pvalues=avg_pvalues,\n",
    "                threshold_value=STAT_PARAMS['significance_threshold'],\n",
    "                method='pvalue'\n",
    "            )\n",
    "            \n",
    "            if thresh_info_pval['success']:\n",
    "                print(f\"   ✓ P-value thresholding (p < {STAT_PARAMS['significance_threshold']})\")\n",
    "                print(f\"     - Original connections: {thresh_info_pval['n_original']}\")\n",
    "                print(f\"     - Surviving connections: {thresh_info_pval['n_surviving']}\")\n",
    "                print(f\"     - Survival rate: {thresh_info_pval['fraction_surviving']*100:.1f}%\")\n",
    "        \n",
    "        # Method 2: FDR-based thresholding (using significance mask)\n",
    "        if sig_mask is not None:\n",
    "            thresh_fdr, thresh_info_fdr = threshold_gc_matrix(\n",
    "                avg_gc_matrix,\n",
    "                significant_mask=sig_mask,\n",
    "                method='mask'\n",
    "            )\n",
    "            \n",
    "            if thresh_info_fdr['success']:\n",
    "                print(f\"   ✓ FDR-based thresholding\")\n",
    "                print(f\"     - Surviving connections: {thresh_info_fdr['n_surviving']}\")\n",
    "                print(f\"     - Survival rate: {thresh_info_fdr['fraction_surviving']*100:.1f}%\")\n",
    "        else:\n",
    "            thresh_fdr = None\n",
    "        \n",
    "        # Method 3: Permutation-based thresholding\n",
    "        if thresh_95 is not None:\n",
    "            thresh_perm, thresh_info_perm = threshold_gc_matrix(\n",
    "                avg_gc_matrix,\n",
    "                threshold_value=thresh_95,\n",
    "                method='absolute'\n",
    "            )\n",
    "            \n",
    "            if thresh_info_perm['success']:\n",
    "                print(f\"   ✓ Permutation-based thresholding (95th percentile)\")\n",
    "                print(f\"     - Threshold: {thresh_95:.4f}\")\n",
    "                print(f\"     - Surviving connections: {thresh_info_perm['n_surviving']}\")\n",
    "                print(f\"     - Survival rate: {thresh_info_perm['fraction_surviving']*100:.1f}%\")\n",
    "        else:\n",
    "            thresh_perm = None\n",
    "        \n",
    "        # Step 4: Visualize results\n",
    "        print(f\"\\n5. Creating comparison visualizations...\")\n",
    "        \n",
    "        try:\n",
    "            # Determine how many thresholded matrices we have\n",
    "            thresh_matrices = []\n",
    "            thresh_titles = []\n",
    "            \n",
    "            thresh_matrices.append(avg_gc_matrix)\n",
    "            thresh_titles.append('Original GC Matrix')\n",
    "            \n",
    "            if 'thresh_pval' in locals() and thresh_pval is not None:\n",
    "                thresh_matrices.append(thresh_pval)\n",
    "                thresh_titles.append(f'P-value Threshold (p<{STAT_PARAMS[\"significance_threshold\"]})')\n",
    "            \n",
    "            if thresh_fdr is not None:\n",
    "                thresh_matrices.append(thresh_fdr)\n",
    "                thresh_titles.append(f'FDR Corrected ({STAT_PARAMS[\"correction_method\"]})')\n",
    "            \n",
    "            if thresh_perm is not None:\n",
    "                thresh_matrices.append(thresh_perm)\n",
    "                thresh_titles.append('Permutation Threshold (95%)')\n",
    "            \n",
    "            n_plots = len(thresh_matrices)\n",
    "            \n",
    "            # Create figure\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "            fig.suptitle(f'Statistical Testing Comparison - {test_subject}/{test_session}', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            \n",
    "            axes_flat = axes.flatten()\n",
    "            \n",
    "            # Plot each thresholded matrix\n",
    "            for idx, (matrix, title) in enumerate(zip(thresh_matrices[:4], thresh_titles[:4])):\n",
    "                ax = axes_flat[idx]\n",
    "                \n",
    "                im = ax.imshow(matrix, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "                ax.set_title(title)\n",
    "                ax.set_xlabel('Source Channel')\n",
    "                ax.set_ylabel('Target Channel')\n",
    "                plt.colorbar(im, ax=ax, label='GC F-statistic')\n",
    "                \n",
    "                # Add text with connection count\n",
    "                n_connections = np.sum(matrix > 0) - matrix.shape[0]  # Exclude diagonal\n",
    "                ax.text(0.02, 0.98, f'Connections: {n_connections}', \n",
    "                       transform=ax.transAxes, fontsize=10, \n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for idx in range(n_plots, 4):\n",
    "                axes_flat[idx].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            stat_plot_path = OUTPUT_DIRS['plots'] / f'{test_subject}_{test_session}_statistical_testing.png'\n",
    "            plt.savefig(stat_plot_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"   ✓ Saved statistical testing plot to: {stat_plot_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ Could not create visualization: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # Additional visualization: Null distribution\n",
    "        if null_dist is not None:\n",
    "            print(f\"\\n6. Visualizing null distribution...\")\n",
    "            \n",
    "            try:\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "                \n",
    "                # Plot 1: Null distribution histogram\n",
    "                ax = axes[0]\n",
    "                ax.hist(null_dist, bins=50, color='lightgray', edgecolor='black', alpha=0.7, density=True)\n",
    "                \n",
    "                # Add vertical lines for thresholds\n",
    "                if thresh_95 is not None:\n",
    "                    ax.axvline(thresh_95, color='orange', linestyle='--', linewidth=2, \n",
    "                              label=f'95th percentile: {thresh_95:.4f}')\n",
    "                if thresh_99 is not None:\n",
    "                    ax.axvline(thresh_99, color='red', linestyle='--', linewidth=2,\n",
    "                              label=f'99th percentile: {thresh_99:.4f}')\n",
    "                \n",
    "                # Add observed GC distribution\n",
    "                observed_gc = avg_gc_matrix[avg_gc_matrix > 0]\n",
    "                ax.hist(observed_gc, bins=50, color='steelblue', edgecolor='black', \n",
    "                       alpha=0.5, density=True, label='Observed GC')\n",
    "                \n",
    "                ax.set_xlabel('GC F-statistic')\n",
    "                ax.set_ylabel('Density')\n",
    "                ax.set_title('Null Distribution vs Observed GC')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Plot 2: Q-Q plot\n",
    "                ax = axes[1]\n",
    "                from scipy import stats as scipy_stats\n",
    "                scipy_stats.probplot(null_dist, dist=\"norm\", plot=ax)\n",
    "                ax.set_title('Q-Q Plot of Null Distribution')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save\n",
    "                null_plot_path = OUTPUT_DIRS['plots'] / f'{test_subject}_{test_session}_null_distribution.png'\n",
    "                plt.savefig(null_plot_path, dpi=150, bbox_inches='tight')\n",
    "                print(f\"   ✓ Saved null distribution plot to: {null_plot_path}\")\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ✗ Could not create null distribution plot: {e}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del raw, raw_prep, segments\n",
    "    \n",
    "    else:\n",
    "        print(\"✗ No segments available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3fdbe",
   "metadata": {},
   "source": [
    "## Step 8 Summary: Statistical Testing\n",
    "\n",
    "We have successfully implemented the complete statistical testing framework for Granger causality analysis:\n",
    "\n",
    "### Functions Implemented\n",
    "\n",
    "1. **`permutation_test_gc()`**\n",
    "   - Establishes significance thresholds via null distribution\n",
    "   - Uses random circular shifts to preserve temporal structure\n",
    "   - Returns 95th and 99th percentile thresholds\n",
    "   - Configurable number of permutations (default: 1000)\n",
    "\n",
    "2. **`apply_fdr_correction()`**\n",
    "   - Controls false discovery rate in multiple comparisons\n",
    "   - Supports FDR-BH (Benjamini-Hochberg) and Bonferroni methods\n",
    "   - Returns corrected p-values and significance mask\n",
    "   - Reports reduction in significant connections\n",
    "\n",
    "3. **`threshold_gc_matrix()`**\n",
    "   - Zeros out non-significant connections\n",
    "   - Four thresholding methods:\n",
    "     - P-value based (e.g., p < 0.05)\n",
    "     - FDR corrected (using significance mask)\n",
    "     - Permutation-based (absolute GC value)\n",
    "     - Custom mask\n",
    "   - Returns thresholded matrix with statistics\n",
    "\n",
    "### Testing Results\n",
    "\n",
    "The test demonstrated all three functions on sample data from the first subject:\n",
    "\n",
    "- **Permutation test**: Established null distribution and significance thresholds\n",
    "- **FDR correction**: Applied multiple comparison correction to p-values\n",
    "- **Thresholding**: Compared different thresholding strategies side-by-side\n",
    "\n",
    "### Visualizations Created\n",
    "\n",
    "1. **Statistical Testing Comparison**: 4-panel visualization showing original vs thresholded GC matrices\n",
    "2. **Null Distribution**: Histogram of permutation-based null distribution with thresholds\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
